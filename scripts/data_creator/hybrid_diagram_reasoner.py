#!/usr/bin/env python3
"""
Hybrid Diagram Figure Reasoner - æ”¯æŒå¤šAPIæºçš„æ™ºèƒ½å›¾è¡¨åˆ†æå™¨

åŠŸèƒ½ï¼š
1. æ”¯æŒåˆ‡æ¢ä¸åŒçš„GPT-4oæ¥å£æºï¼ˆPapyrus å’Œ Azure OpenAIï¼‰
2. æå–æ‰€æœ‰å›¾ç‰‡çš„caption
3. ä½¿ç”¨GPTåˆ¤æ–­å“ªäº›æ˜¯diagram
4. åªåˆ†æè¢«GPTè¯†åˆ«ä¸ºdiagramçš„å›¾ç‰‡
5. ç”Ÿæˆç»˜å›¾æŒ‡ä»¤

æ”¯æŒçš„APIæºï¼š
- Papyrus (Microsoftå†…éƒ¨APIï¼Œé»˜è®¤)
- Azure OpenAI (å¤‡é€‰)
"""

import os
import re
import json
import requests
import base64
from pathlib import Path
from typing import Dict, List, Optional
from enum import Enum

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, AzureOpenAIEmbeddings
from prompts_template.prompt_manager import prompt_manager
from langchain.schema import Document
HAS_LANGCHAIN = True

from dotenv import load_dotenv
HAS_DOTENV = True

# Azure Identity imports for Papyrus
try:
    from azure.identity import DefaultAzureCredential, AzureCliCredential, ManagedIdentityCredential
    HAS_AZURE_IDENTITY = True
except ImportError:
    HAS_AZURE_IDENTITY = False
    print("âš ï¸ Azure Identity not available. Papyrus API will not work.")


class APISource(Enum):
    """APIæºæšä¸¾"""
    AZURE_OPENAI = "azure_openai"
    PAPYRUS = "papyrus"


class HybridDiagramReasoner:
    """æ··åˆå›¾è¡¨åˆ†æå™¨ï¼Œæ”¯æŒå¤šAPIæº"""
    
    def __init__(self, api_source: APISource = APISource.PAPYRUS):
        self.api_source = api_source
        self.load_env_vars()
        self.setup_api_config()
    
    def load_env_vars(self):
        """åŠ è½½ç¯å¢ƒå˜é‡"""
        # å°è¯•ä»å¤šä¸ªä½ç½®åŠ è½½ .env æ–‡ä»¶
        env_paths = [
            Path(__file__).parent / ".env",
            Path(__file__).parent.parent / ".env",
            Path(__file__).parent.parent.parent / "CoTT" / ".env",
            Path(__file__).parent.parent.parent / "CoTT" / ".env_old",
            Path("/Users/suny0a/Proj/MM-Reasoning/CoTT/.env"),  # ç»å¯¹è·¯å¾„
            Path("/home/t2vg-a100-G2-0/yasheng/CoTT/.env"),  # æœåŠ¡å™¨è·¯å¾„
        ]
        
        if HAS_DOTENV:
            for env_path in env_paths:
                if env_path.exists():
                    print(f"ğŸ“„ åŠ è½½ç¯å¢ƒå˜é‡: {env_path}")
                    load_dotenv(env_path)
                    # éªŒè¯å…³é”®ç¯å¢ƒå˜é‡æ˜¯å¦åŠ è½½æˆåŠŸ
                    api_key = os.getenv("AZURE_OPENAI_API_KEY")
                    if api_key:
                        print(f"âœ… Azure OpenAI API Key å·²åŠ è½½: {api_key[:20]}...")
                    else:
                        print("âŒ Azure OpenAI API Key æœªæ‰¾åˆ°")
                    return True
        else:
            # ç®€å•çš„ç¯å¢ƒå˜é‡åŠ è½½
            for env_path in env_paths:
                if env_path.exists():
                    print(f"ğŸ“„ ä»æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡: {env_path}")
                    with open(env_path, 'r') as f:
                        for line in f:
                            line = line.strip()
                            if line and not line.startswith('#') and '=' in line:
                                key, value = line.split('=', 1)
                                os.environ[key] = value
                    
                    # éªŒè¯å…³é”®ç¯å¢ƒå˜é‡æ˜¯å¦åŠ è½½æˆåŠŸ
                    api_key = os.getenv("AZURE_OPENAI_API_KEY")
                    if api_key:
                        print(f"âœ… Azure OpenAI API Key å·²åŠ è½½: {api_key[:20]}...")
                    else:
                        print("âŒ Azure OpenAI API Key æœªæ‰¾åˆ°")
                    return True
        
        print("âš ï¸ æœªæ‰¾åˆ°ç¯å¢ƒå˜é‡æ–‡ä»¶")
        return False
    
    def setup_api_config(self):
        """è®¾ç½®APIé…ç½®"""
        if self.api_source == APISource.AZURE_OPENAI:
            self.endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "https://linjl-ma65uv6u-eastus2.cognitiveservices.azure.com/")
            self.api_key = os.getenv("AZURE_OPENAI_API_KEY", "")
            self.deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
            self.api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
            
            if not self.api_key:
                print("âŒ Azure OpenAI API key not found")
                return False
            
            print(f"ğŸ”— ä½¿ç”¨ Azure OpenAI API")
            print(f"   Endpoint: {self.endpoint}")
            print(f"   Deployment: {self.deployment}")
            
        elif self.api_source == APISource.PAPYRUS:
            if not HAS_AZURE_IDENTITY:
                print("âŒ Azure Identity not available for Papyrus API")
                return False
            
            # ä»ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼è·å–Papyrusé…ç½®
            self.papyrus_endpoint = os.getenv("PAPYRUS_ENDPOINT", "https://WestUS2Large.papyrus.binginternal.com/chat/completions")
            self.verify_scope = os.getenv("PAPYRUS_VERIFY_SCOPE", "api://5fe538a8-15d5-4a84-961e-be66cd036687/.default")
            self.client_id = os.getenv("PAPYRUS_CLIENT_ID", "d5702df1-96d9-4195-83a3-e44d8b0a0601")
            
            # å°è¯•ä¸åŒçš„è®¤è¯æ–¹å¼
            self.access_token = None
            self.setup_papyrus_auth()
            
            if not self.access_token:
                print("âŒ Failed to get Papyrus access token")
                return False
            
            print(f"ğŸ”— ä½¿ç”¨ Papyrus API")
            print(f"   Endpoint: {self.papyrus_endpoint}")
            print(f"   Access token: {self.access_token[:20]}...")
        
        return True
    
    def setup_papyrus_auth(self):
        """è®¾ç½®Papyrusè®¤è¯"""
        # ä¼˜å…ˆå°è¯•ManagedIdentityCredentialï¼ˆä¸papyrus_on_vm_2.pyä¿æŒä¸€è‡´ï¼‰
        try:
            print("ğŸ” å°è¯•ä½¿ç”¨ Managed Identity è®¤è¯...")
            cred = ManagedIdentityCredential(client_id=self.client_id)
            self.access_token = cred.get_token(self.verify_scope).token
            print("âœ… Managed Identity è®¤è¯æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ Managed Identity è®¤è¯å¤±è´¥: {e}")
        
        try:
            # å°è¯•ä½¿ç”¨DefaultAzureCredential
            print("ğŸ” å°è¯•ä½¿ç”¨ Default Azure è®¤è¯...")
            cred = DefaultAzureCredential()
            self.access_token = cred.get_token(self.verify_scope).token
            print("âœ… Default Azure è®¤è¯æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ Default Azure è®¤è¯å¤±è´¥: {e}")
        
        try:
            # æœ€åå°è¯•ä½¿ç”¨AzureCliCredential
            print("ğŸ” å°è¯•ä½¿ç”¨ Azure CLI è®¤è¯...")
            cred = AzureCliCredential()
            self.access_token = cred.get_token(self.verify_scope).token
            print("âœ… Azure CLI è®¤è¯æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ Azure CLI è®¤è¯å¤±è´¥: {e}")
        
        return False
    
    def get_api_headers(self) -> Dict[str, str]:
        """è·å–APIè¯·æ±‚å¤´"""
        if self.api_source == APISource.AZURE_OPENAI:
            return {
                "Content-Type": "application/json",
                "api-key": self.api_key
            }
        elif self.api_source == APISource.PAPYRUS:
            return {
                "Authorization": f"Bearer {self.access_token}",
                "Content-Type": "application/json",
                "papyrus-model-name": os.getenv("PAPYRUS_MODEL_NAME", "gpt4ovision-batch"),
                "papyrus-timeout-ms": os.getenv("PAPYRUS_TIMEOUT_MS", "30000"),
                "papyrus-quota-id": os.getenv("PAPYRUS_QUOTA_ID", "msftaicopilot/windowsdata"),
            }
        return {}
    
    def get_api_url(self) -> str:
        """è·å–APIè¯·æ±‚URL"""
        if self.api_source == APISource.AZURE_OPENAI:
            return f"{self.endpoint}openai/deployments/{self.deployment}/chat/completions?api-version={self.api_version}"
        elif self.api_source == APISource.PAPYRUS:
            return self.papyrus_endpoint
        return ""
    
    def extract_all_figures_from_markdown(self, markdown_content: str) -> List[Dict]:
        """æå–æ‰€æœ‰å›¾ç‰‡ä¿¡æ¯ï¼Œä¸åšä»»ä½•è¿‡æ»¤"""
        figures = []
        seen_images = set()  # é¿å…é‡å¤æå–
        
        # æ‰¾åˆ°é™„å½•å¼€å§‹çš„ä½ç½®ï¼ˆæ’é™¤é™„å½•å†…å®¹ï¼‰
        lines = markdown_content.split('\n')
        appendix_start_idx = len(lines)  # é»˜è®¤æ²¡æœ‰é™„å½•
        for i, line in enumerate(lines):
            line_lower = line.lower().strip()
            # æ£€æŸ¥æ˜¯å¦æ˜¯é™„å½•æ ‡é¢˜
            if (line_lower.startswith('# appendix') or 
                line_lower.startswith('## appendix') or
                line_lower.startswith('### appendix') or
                line_lower.startswith('# supplementary') or
                line_lower.startswith('## supplementary') or
                line_lower.startswith('### supplementary') or
                'appendix' in line_lower and line_lower.startswith('#')):
                appendix_start_idx = i
                break
        
        # åªå¤„ç†æ­£æ–‡éƒ¨åˆ†ï¼ˆæ’é™¤é™„å½•ï¼‰
        main_content = '\n'.join(lines[:appendix_start_idx])
        
        # æŸ¥æ‰¾markdownæ ¼å¼çš„å›¾ç‰‡å¼•ç”¨: ![](path) æˆ– ![alt](path)
        image_pattern = r'!\[([^\]]*)\]\(([^)]+\.(?:jpg|jpeg|png|gif|bmp|svg))\)'
        image_matches = re.findall(image_pattern, main_content, re.IGNORECASE)
        
        # å¤„ç†æ¯ä¸ªå›¾ç‰‡å¼•ç”¨
        for alt_text, image_path in image_matches:
            # é¿å…é‡å¤å¤„ç†
            if image_path in seen_images:
                continue
            seen_images.add(image_path)
            
            # æŸ¥æ‰¾è¿™ä¸ªå›¾ç‰‡åé¢çš„caption
            caption = ""
            image_pos = main_content.find(f"![{alt_text}]({image_path})")
            if image_pos != -1:
                # æŸ¥æ‰¾å›¾ç‰‡åçš„æ–‡æœ¬ä½œä¸ºcaption
                after_image = main_content[image_pos + len(f"![{alt_text}]({image_path})"):]
                # æŸ¥æ‰¾ä¸‹ä¸€è¡Œæˆ–æ®µè½ä½œä¸ºcaption
                lines_after = after_image.split('\n')
                for line in lines_after:
                    line = line.strip()
                    if line and not line.startswith('![') and not line.startswith('#'):
                        # æ£€æŸ¥æ˜¯å¦æ˜¯Figureå¼€å¤´çš„caption
                        if line.lower().startswith('figure'):
                            caption = line
                            break
                        # æˆ–è€…å–ç¬¬ä¸€è¡Œéç©ºæ–‡æœ¬ä½œä¸ºcaption
                        elif not caption:
                            caption = line
                        # å¦‚æœé‡åˆ°ä¸‹ä¸€ä¸ªå›¾ç‰‡æˆ–æ ‡é¢˜ï¼Œåœæ­¢
                        if line.startswith('![') or line.startswith('#'):
                            break
            
            figures.append({
                'id': f"figure_{len(figures) + 1}",
                'src': image_path,
                'caption': caption or alt_text or f"Figure {len(figures) + 1}",
                'alt_text': alt_text
            })
        
        return figures
    
    def classify_figures_with_gpt(self, figures: List[Dict], paper_title: str = "") -> List[Dict]:
        """ä½¿ç”¨GPTåˆ¤æ–­å“ªäº›å›¾ç‰‡æ˜¯diagram"""
        
        if not self.setup_api_config():
            print("âŒ APIé…ç½®å¤±è´¥")
            return []
        
        # æ„å»ºæ‰€æœ‰å›¾ç‰‡çš„captionä¿¡æ¯
        figure_info = []
        for i, fig in enumerate(figures, 1):
            figure_info.append(f"Figure {i}: {fig['caption']}")
        
        figures_text = "\n".join(figure_info)
        
        # ä½¿ç”¨promptæ¨¡æ¿
        prompt = prompt_manager.get_figure_classification_prompt(paper_title, figures_text)

        payload = {
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "max_tokens": 1000,
            "temperature": 0.1
        }
        
        try:
            url = self.get_api_url()
            headers = self.get_api_headers()
            
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            
            data = response.json()
            content = data.get("choices", [{}])[0].get("message", {}).get("content", "{}")
            
            # å°è¯•è§£æJSON
            try:
                # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
                result = json.loads(content)
            except json.JSONDecodeError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æå–markdownä»£ç å—ä¸­çš„JSON
                try:
                    # æŸ¥æ‰¾```jsonå’Œ```ä¹‹é—´çš„å†…å®¹
                    json_match = re.search(r'```json\s*\n(.*?)\n```', content, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1)
                        result = json.loads(json_content)
                    else:
                        # å°è¯•æŸ¥æ‰¾```å’Œ```ä¹‹é—´çš„å†…å®¹ï¼ˆæ²¡æœ‰jsonæ ‡è®°ï¼‰
                        code_match = re.search(r'```\s*\n(.*?)\n```', content, re.DOTALL)
                        if code_match:
                            json_content = code_match.group(1)
                            result = json.loads(json_content)
                        else:
                            print(f"   âŒ æ— æ³•æ‰¾åˆ°JSONå†…å®¹: {content}")
                            return []
                except json.JSONDecodeError:
                    print(f"   âŒ æ— æ³•è§£æGPTåˆ†ç±»ç»“æœ: {content}")
                    return []
            
            # è§£ææˆåŠŸï¼Œæå–ç»“æœ
            diagram_figure_numbers = result.get("diagram_figures", [])
            reasoning = result.get("reasoning", "")
            
            print(f"   ğŸ¤– GPTåˆ†ç±»ç»“æœ: {reasoning}")
            
            # æ ¹æ®GPTçš„åˆ†ç±»ç»“æœç­›é€‰å›¾ç‰‡
            diagram_figures = []
            for i, fig in enumerate(figures, 1):
                if i in diagram_figure_numbers:
                    fig['type'] = 'diagram'
                    fig['gpt_reasoning'] = reasoning
                    diagram_figures.append(fig)
            
            return diagram_figures
                
        except Exception as e:
            print(f"   âŒ GPTåˆ†ç±»å¤±è´¥: {str(e)}")
            return []
    
    def encode_image(self, image_path: str) -> str:
        """å°†å›¾ç‰‡ç¼–ç ä¸ºbase64"""
        try:
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')
        except Exception as e:
            print(f"   âŒ å›¾ç‰‡ç¼–ç å¤±è´¥: {e}")
            return ""
    
    def analyze_diagram_with_gpt4o(self, image_path: str, caption: str, context: str) -> Dict:
        """ä½¿ç”¨GPT-4oåˆ†ædiagramå›¾ç‰‡å†…å®¹"""
        
        if not self.setup_api_config():
            return {"error": "APIé…ç½®å¤±è´¥"}
        
        # ç¼–ç å›¾ç‰‡
        base64_image = self.encode_image(image_path)
        if not base64_image:
            return {"error": "Failed to encode image"}
        
        # ä½¿ç”¨promptæ¨¡æ¿
        prompt = prompt_manager.get_diagram_analysis_prompt(caption, context)

        payload = {
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": prompt
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            "max_tokens": 3000,
            "temperature": 0.1
        }
        
        try:
            url = self.get_api_url()
            headers = self.get_api_headers()
            
            response = requests.post(url, headers=headers, json=payload, timeout=180)
            response.raise_for_status()
            
            data = response.json()
            content = data.get("choices", [{}])[0].get("message", {}).get("content", "{}")
            
            # å°è¯•è§£æJSON
            try:
                # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
                result = json.loads(content)
                return result
            except json.JSONDecodeError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æå–markdownä»£ç å—ä¸­çš„JSON
                try:
                    # æŸ¥æ‰¾```jsonå’Œ```ä¹‹é—´çš„å†…å®¹
                    json_match = re.search(r'```json\s*\n(.*?)\n```', content, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1)
                        result = json.loads(json_content)
                        return result
                    else:
                        # å°è¯•æŸ¥æ‰¾```å’Œ```ä¹‹é—´çš„å†…å®¹ï¼ˆæ²¡æœ‰jsonæ ‡è®°ï¼‰
                        code_match = re.search(r'```\s*\n(.*?)\n```', content, re.DOTALL)
                        if code_match:
                            json_content = code_match.group(1)
                            result = json.loads(json_content)
                            return result
                        else:
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»£ç å—ï¼Œå°è¯•æ¸…ç†å†…å®¹åè§£æ
                            cleaned_content = content.strip()
                            # ç§»é™¤å¯èƒ½çš„markdownæ ¼å¼
                            cleaned_content = re.sub(r'^```.*?\n', '', cleaned_content, flags=re.DOTALL)
                            cleaned_content = re.sub(r'\n```.*?$', '', cleaned_content, flags=re.DOTALL)
                            result = json.loads(cleaned_content)
                            return result
                except json.JSONDecodeError:
                    # å¦‚æœè¿˜æ˜¯è§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹ç”¨äºè°ƒè¯•
                    print(f"   âš ï¸ JSONè§£æå¤±è´¥ï¼ŒåŸå§‹å“åº”: {content[:200]}...")
                    return {
                        "raw_response": content,
                        "error": "Failed to parse JSON response",
                        "diagram_analysis_available": False
                    }
                
        except Exception as e:
            return {"error": f"API request failed: {str(e)}"}
    
    def switch_api_source(self, new_source: APISource):
        """åˆ‡æ¢APIæº"""
        print(f"ğŸ”„ åˆ‡æ¢APIæº: {self.api_source.value} -> {new_source.value}")
        self.api_source = new_source
        return self.setup_api_config()
    
    def test_api_connection(self) -> bool:
        """æµ‹è¯•APIè¿æ¥"""
        print(f"ğŸ” æµ‹è¯• {self.api_source.value} APIè¿æ¥...")
        
        test_payload = {
            "messages": [
                {
                    "role": "user",
                    "content": "Hello, this is a test message."
                }
            ],
            "max_tokens": 10,
            "temperature": 0.1
        }
        
        try:
            url = self.get_api_url()
            headers = self.get_api_headers()
            
            response = requests.post(url, headers=headers, json=test_payload, timeout=30)
            response.raise_for_status()
            
            print(f"âœ… {self.api_source.value} APIè¿æ¥æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ {self.api_source.value} APIè¿æ¥å¤±è´¥: {str(e)}")
            return False


def extract_json_from_markdown(content: str) -> Dict:
    """ä»markdownå†…å®¹ä¸­æå–JSON"""
    try:
        # æŸ¥æ‰¾```jsonå’Œ```ä¹‹é—´çš„å†…å®¹
        json_match = re.search(r'```json\s*\n(.*?)\n```', content, re.DOTALL)
        if json_match:
            json_content = json_match.group(1)
            return json.loads(json_content)
        
        # å°è¯•æŸ¥æ‰¾```å’Œ```ä¹‹é—´çš„å†…å®¹ï¼ˆæ²¡æœ‰jsonæ ‡è®°ï¼‰
        code_match = re.search(r'```\s*\n(.*?)\n```', content, re.DOTALL)
        if code_match:
            json_content = code_match.group(1)
            return json.loads(json_content)
        
        # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥è§£æ
        return json.loads(content)
        
    except json.JSONDecodeError:
        return {}




def show_setup_help():
    """æ˜¾ç¤ºè®¾ç½®å¸®åŠ©ä¿¡æ¯"""
    print("\nğŸ”§ è®¾ç½®å¸®åŠ©")
    print("=" * 50)
    print("æ­¤å·¥å…·é»˜è®¤ä½¿ç”¨ Papyrus API (Microsoftå†…éƒ¨API)")
    print("è¦ä½¿ç”¨æ­¤å·¥å…·ï¼Œéœ€è¦è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡:")
    print()
    print("1. Papyrus API (é»˜è®¤ï¼Œæ¨è):")
    print("   export PAPYRUS_ENDPOINT='https://WestUS2Large.papyrus.binginternal.com/chat/completions'")
    print("   export PAPYRUS_VERIFY_SCOPE='api://5fe538a8-15d5-4a84-961e-be66cd036687/.default'")
    print("   export PAPYRUS_CLIENT_ID='d5702df1-96d9-4195-83a3-e44d8b0a0601'")
    print()
    print("2. å¤‡é€‰ Azure OpenAI API:")
    print("   export AZURE_OPENAI_ENDPOINT='https://your-endpoint.cognitiveservices.azure.com/'")
    print("   export AZURE_OPENAI_API_KEY='your-api-key-here'")
    print("   export AZURE_OPENAI_DEPLOYMENT='gpt-4o'")
    print()
    print("3. å¯¹äºPapyrus APIï¼Œéœ€è¦å®‰è£…:")
    print("   pip install azure-identity")
    print()
    print("4. å¦‚æœAzure CLI tokenè¿‡æœŸï¼Œé‡æ–°ç™»å½•:")
    print("   az login")
    print()


def test_smart_markdown_paper(paper_dir: Path, paper_name: str, reasoner: HybridDiagramReasoner):
    """æµ‹è¯•å•ä¸ªmarkdownè®ºæ–‡çš„æ™ºèƒ½diagramåˆ†æ"""
    print(f"\n{'='*60}")
    print(f"ğŸ“š æµ‹è¯•è®ºæ–‡: {paper_name}")
    print(f"{'='*60}")
    
    # æŸ¥æ‰¾markdownæ–‡ä»¶
    markdown_path = paper_dir / "vlm" / f"{paper_name}.md"
    if not markdown_path.exists():
        print(f"âŒ æœªæ‰¾åˆ°markdownæ–‡ä»¶: {markdown_path}")
        return None
    
    print(f"ğŸ“„ è¯»å–markdownæ–‡ä»¶: {markdown_path}")
    try:
        with open(markdown_path, 'r', encoding='utf-8') as f:
            markdown_content = f.read()
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return None
    
    # æå–æ‰€æœ‰å›¾ç‰‡
    print("\nğŸ” æå–æ‰€æœ‰å›¾ç‰‡...")
    all_figures = reasoner.extract_all_figures_from_markdown(markdown_content)
    
    if not all_figures:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡")
        return {
            "paper_name": paper_name,
            "paper_dir": str(paper_dir),
            "total_figures": 0,
            "diagram_figures": 0,
            "diagram_figures_list": [],
            "results": []
        }
    
    print(f"âœ… æ‰¾åˆ° {len(all_figures)} ä¸ªå›¾ç‰‡")
    
    # ä½¿ç”¨GPTæ™ºèƒ½åˆ†ç±»å›¾ç‰‡
    print("\nğŸ¤– ä½¿ç”¨GPTæ™ºèƒ½åˆ†ç±»å›¾ç‰‡...")
    diagram_figures = reasoner.classify_figures_with_gpt(all_figures, paper_name)
    
    if not diagram_figures:
        print("âŒ GPTæœªè¯†åˆ«å‡ºä»»ä½•diagramå›¾ç‰‡")
        return {
            "paper_name": paper_name,
            "paper_dir": str(paper_dir),
            "total_figures": len(all_figures),
            "diagram_figures": 0,
            "diagram_figures_list": [],
            "results": []
        }
    
    print(f"âœ… GPTè¯†åˆ«å‡º {len(diagram_figures)} ä¸ªdiagramå›¾ç‰‡")
    
    # åˆ†ææ¯ä¸ªdiagramå›¾ç‰‡
    results = []
    for i, figure in enumerate(diagram_figures, 1):
        print(f"\nğŸ“Š åˆ†ædiagramå›¾ç‰‡ {i}: {figure['id']}")
        print(f"   Caption: {figure['caption'][:100]}...")
        
        # æ„å»ºå›¾ç‰‡è·¯å¾„
        image_path = paper_dir / "vlm" / figure['src']
        
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not image_path.exists():
            print(f"   âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
            possible_paths = [
                paper_dir / "vlm" / "images" / figure['src'],
                paper_dir / "vlm" / figure['src'].replace('images/', ''),
                paper_dir / figure['src'],
                paper_dir / "images" / figure['src']
            ]
            
            found = False
            for possible_path in possible_paths:
                if possible_path.exists():
                    image_path = possible_path
                    found = True
                    print(f"   âœ… æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶: {image_path}")
                    break
            
            if not found:
                print(f"   âŒ æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡è·¯å¾„éƒ½ä¸å­˜åœ¨")
                continue
        
        # ä½¿ç”¨æ£€ç´¢æå–ç›¸å…³ä¸Šä¸‹æ–‡
        full_context = get_semantic_context_for_figure(markdown_content, figure['caption'])
        
        # è¾“å‡ºæå–åˆ°çš„ä¸Šä¸‹æ–‡ç”¨äºè°ƒè¯•
        print(f"   ğŸ“„ æå–åˆ°çš„ä¸Šä¸‹æ–‡:")
        print(f"   {'='*50}")
        print(f"   {full_context[:300]}...")
        print(f"   {'='*50}")
        
        # ä½¿ç”¨GPT-4oåˆ†æå›¾ç‰‡
        print("   ğŸ” ä½¿ç”¨GPT-4oåˆ†æå›¾ç‰‡...")
        diagram_analysis = reasoner.analyze_diagram_with_gpt4o(str(image_path), figure['caption'], full_context)
        
        if "error" in diagram_analysis:
            print(f"   âŒ å›¾ç‰‡åˆ†æå¤±è´¥: {diagram_analysis['error']}")
            # å¦‚æœåªæ˜¯JSONè§£æå¤±è´¥ï¼Œä½†ä»ç„¶æœ‰åŸå§‹å“åº”ï¼Œå¯ä»¥ç»§ç»­å¤„ç†
            if diagram_analysis.get("raw_response"):
                print(f"   âš ï¸ ä½†æœ‰åŸå§‹å“åº”å¯ç”¨ï¼Œç»§ç»­å¤„ç†...")
            else:
                continue
        
        print("   âœ… å›¾ç‰‡åˆ†æå®Œæˆ")
        
        # æ„å»ºè®­ç»ƒæ•°æ®
        training_data = {
            "data_quality": "valid",
            "quality_issues": [],
            "stage1_input": {
                "context": full_context,
                "caption": figure['caption']
            },
            "stage2_input": {
                "diagram_description": diagram_analysis.get("diagram_analysis", {}).get("diagram_type", ""),
                "main_components": diagram_analysis.get("diagram_analysis", {}).get("nodes", [])
            },
            "stage2_output": {
                "thinking": "",
                "image_path": str(image_path)
            }
        }
        
        # æ„å»ºjudgeæ•°æ®
        judge_data = {
            "image_info": {
                "image_path": str(image_path),
                "figure_id": figure['id'],
                "figure_src": figure['src'],
                "figure_caption": figure['caption']
            },
            "evaluation_rubric": {
                "semantic_criteria": {
                    "critical_entities": diagram_analysis.get("diagram_analysis", {}).get("nodes", []),
                    "critical_relationships": diagram_analysis.get("diagram_analysis", {}).get("relationships", []),
                    "hierarchical_groups": diagram_analysis.get("diagram_analysis", {}).get("groups", []),
                    "data_flow": "Sequential processing flow",
                    "dependencies": []
                },
                "visual_criteria": {
                    "layout_requirements": "",
                    "color_scheme": [],
                    "shape_requirements": []
                }
            },
            "reference_descriptions": {
                "detailed_thinking": "",
                "concise_thinking": ""
            }
        }
        
        result = {
            "training_data": training_data,
            "judge_data": judge_data
        }
        results.append(result)
    
    return {
        "paper_name": paper_name,
        "paper_dir": str(paper_dir),
        "total_figures": len(all_figures),
        "diagram_figures": len(diagram_figures),
        "diagram_figures_list": diagram_figures,
        "results": results
    }


def get_semantic_context_for_figure(markdown_content: str, caption: str) -> str:
    """ä½¿ç”¨FAISSæ£€ç´¢æ‰¾åˆ°ä¸å›¾ç‰‡ç›¸å…³çš„ä¸Šä¸‹æ–‡å†…å®¹"""
    if not HAS_LANGCHAIN:
        print("   âŒ langchain æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨FAISSæ£€ç´¢")
        return ""
    
    lines = markdown_content.split('\n')
    
    # æ‰¾åˆ°é™„å½•å¼€å§‹çš„ä½ç½®ï¼ˆæ’é™¤é™„å½•å†…å®¹ï¼‰
    appendix_start_idx = len(lines)  # é»˜è®¤æ²¡æœ‰é™„å½•
    for i, line in enumerate(lines):
        line_lower = line.lower().strip()
        # æ£€æŸ¥æ˜¯å¦æ˜¯é™„å½•æ ‡é¢˜
        if (line_lower.startswith('# appendix') or 
            line_lower.startswith('## appendix') or
            line_lower.startswith('### appendix') or
            line_lower.startswith('# supplementary') or
            line_lower.startswith('## supplementary') or
            line_lower.startswith('### supplementary') or
            'appendix' in line_lower and line_lower.startswith('#')):
            appendix_start_idx = i
            break
    
    # å°†æ–‡æ¡£åˆ†å‰²æˆæ®µè½ï¼ˆåªè€ƒè™‘æ­£æ–‡éƒ¨åˆ†ï¼Œæ’é™¤é™„å½•ï¼‰
    paragraphs = []
    current_paragraph = []
    
    for i, line in enumerate(lines[:appendix_start_idx]):  # åªå¤„ç†æ­£æ–‡éƒ¨åˆ†
        line = line.strip()
        if not line:
            if current_paragraph:
                paragraphs.append(' '.join(current_paragraph))
                current_paragraph = []
        elif not line.startswith('![') and not line.startswith('#'):
            current_paragraph.append(line)
    
    # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
    if current_paragraph:
        paragraphs.append(' '.join(current_paragraph))
    
    # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ®µè½
    paragraphs = [text for text in paragraphs if len(text) > 50]
    
    if not paragraphs:
        return ""
    
    # ä½¿ç”¨FAISSæ£€ç´¢
    try:
        print("   ğŸ” ä½¿ç”¨FAISSå¯†é›†æ£€ç´¢...")
        retriever = FAISSRetriever()
        retriever.fit(paragraphs)
        
        # æ„å»ºæŸ¥è¯¢ï¼šä½¿ç”¨captionä½œä¸ºæŸ¥è¯¢ï¼Œæ‰©å±•ç›¸å…³è¯æ±‡
        query = caption
        if any(keyword in caption.lower() for keyword in ['diagram', 'architecture', 'framework', 'pipeline', 'overview', 'structure']):
            query += " system design components modules workflow process"
        print(f"   ğŸ” FAISSæŸ¥è¯¢: {query[:100]}...")
        
        # æœç´¢æœ€ç›¸å…³çš„æ®µè½
        results = retriever.search(query, top_k=5)
        
        # æå–ç›¸å…³æ®µè½
        relevant_contexts = []
        for doc_idx, score in results:
            if score > 0:  # åªä¿ç•™æœ‰åˆ†æ•°çš„ç»“æœ
                text = paragraphs[doc_idx]
                relevant_contexts.append(text)
                print(f"   ğŸ“„ æ‰¾åˆ°ç›¸å…³æ®µè½ (åˆ†æ•°: {score:.3f}): {text[:100]}...")
        
        return '\n\n'.join(relevant_contexts)
        
    except Exception as e:
        print(f"   âŒ FAISSæ£€ç´¢å¤±è´¥: {e}")
        return ""


class FAISSRetriever:
    """åŸºäºFAISSçš„å¯†é›†å‘é‡æ£€ç´¢å™¨"""
    
    def __init__(self):
        self.vector_store = None
        self.documents = []
        self.embeddings = None
        
    def _get_embeddings(self):
        """è·å–embeddingæ¨¡å‹"""
        if self.embeddings is not None:
            return self.embeddings
            
        # æ£€æŸ¥Azure OpenAIé…ç½®
        if os.getenv("AZURE_OPENAI_ENDPOINT") and os.getenv("AZURE_OPENAI_API_KEY"):
            endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
            api_key = os.getenv("AZURE_OPENAI_API_KEY")
            api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-06-01")
            
            self.embeddings = AzureOpenAIEmbeddings(
                azure_endpoint=endpoint,
                openai_api_version=api_version,
                openai_api_key=api_key,
                model="text-embedding-3-large"
            )
            print("   ğŸ”— ä½¿ç”¨ Azure OpenAI Embeddings")
        else:
            # ä½¿ç”¨OpenAI API
            self.embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
            print("   ğŸ”— ä½¿ç”¨ OpenAI Embeddings")
        
        return self.embeddings
    
    def fit(self, documents: List[str]):
        """æ„å»ºFAISSç´¢å¼•"""
        if not HAS_LANGCHAIN:
            raise ImportError("éœ€è¦å®‰è£…langchainæ¥ä½¿ç”¨FAISSæ£€ç´¢")
        
        self.documents = documents
        
        # åˆ›å»ºDocumentå¯¹è±¡
        docs = [Document(page_content=doc) for doc in documents]
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = text_splitter.split_documents(docs)
        
        # è·å–embeddingså¹¶æ„å»ºFAISSç´¢å¼•
        embeddings = self._get_embeddings()
        self.vector_store = FAISS.from_documents(chunks, embeddings)
        
        print(f"   ğŸ“Š FAISSç´¢å¼•æ„å»ºå®Œæˆ: {len(chunks)} ä¸ªchunks")
    
    def search(self, query: str, top_k: int = 3) -> List[tuple]:
        """æœç´¢æœ€ç›¸å…³çš„æ–‡æ¡£"""
        if self.vector_store is None:
            return []
        
        # ä½¿ç”¨FAISSæ£€ç´¢
        retriever = self.vector_store.as_retriever(search_kwargs={"k": top_k})
        try:
            # ä½¿ç”¨æ–°çš„invokeæ–¹æ³•
            docs = retriever.invoke(query)
        except AttributeError:
            # å›é€€åˆ°æ—§æ–¹æ³•
            docs = retriever.get_relevant_documents(query)
        
        # è¿”å›ç»“æœ
        results = []
        for i, doc in enumerate(docs):
            # æ‰¾åˆ°åŸå§‹æ–‡æ¡£çš„ç´¢å¼•
            original_idx = self._find_original_doc_index(doc.page_content)
            if original_idx != -1:
                results.append((original_idx, 1.0 - i * 0.1))  # ç®€å•çš„åˆ†æ•°è®¡ç®—
        
        return results
    
    def _find_original_doc_index(self, content: str) -> int:
        """æ‰¾åˆ°chunkå¯¹åº”çš„åŸå§‹æ–‡æ¡£ç´¢å¼•"""
        for i, doc in enumerate(self.documents):
            if content[:100] in doc:  # ä½¿ç”¨å‰100ä¸ªå­—ç¬¦åŒ¹é…
                return i
        return -1


def main():
    """ä¸»å‡½æ•° - æµ‹è¯•æ‰€æœ‰markdownè®ºæ–‡çš„æ™ºèƒ½diagramåˆ†æ"""
    print("ğŸ¤– æ··åˆå›¾è¡¨åˆ†æå™¨ - æ”¯æŒå¤šAPIæº")
    print("=" * 60)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰APIé…ç½®
    has_azure_key = bool(os.getenv("AZURE_OPENAI_API_KEY"))
    has_azure_identity = HAS_AZURE_IDENTITY
    
    if not has_azure_key and not has_azure_identity:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•APIé…ç½®")
        show_setup_help()
        return
    
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    reasoner = HybridDiagramReasoner(APISource.PAPYRUS)
    
    # æµ‹è¯•APIè¿æ¥
    if not reasoner.test_api_connection():
        print("âŒ APIè¿æ¥å¤±è´¥")
        return
    
    # è®ºæ–‡ç›®å½•
    papers_dir = Path(__file__).parent.parent.parent / "workspace" / "papers_markdown"
    
    # è·å–æ‰€æœ‰è®ºæ–‡ç›®å½•
    paper_dirs = [d for d in papers_dir.iterdir() if d.is_dir()]
    
    if not paper_dirs:
        print("âŒ æœªæ‰¾åˆ°è®ºæ–‡ç›®å½•")
        return
    
    print(f"ğŸ“š æ‰¾åˆ° {len(paper_dirs)} ä¸ªè®ºæ–‡ç›®å½•")
    
    # æµ‹è¯•æ¯ä¸ªè®ºæ–‡
    all_results = []
    for paper_dir in paper_dirs:
        paper_name = paper_dir.name
        result = test_smart_markdown_paper(paper_dir, paper_name, reasoner)
        if result:
            all_results.append(result)
    
    # åˆ†ç¦»è®­ç»ƒæ•°æ®å’Œjudgeæ•°æ®
    training_data = []
    judge_data = []
    
    for result in all_results:
        for item in result["results"]:
            if item.get("training_data"):
                training_data.append(item["training_data"])
            if item.get("judge_data"):
                judge_data.append(item["judge_data"])
    
    # ä¿å­˜è®­ç»ƒæ•°æ®
    training_output_path = Path(__file__).parent / "hybrid_diagram_training_data.json"
    with open(training_output_path, 'w', encoding='utf-8') as f:
        json.dump(training_data, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜judgeæ•°æ®
    judge_output_path = Path(__file__).parent / "hybrid_diagram_judge_data.json"
    with open(judge_output_path, 'w', encoding='utf-8') as f:
        json.dump(judge_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {training_output_path}")
    print(f"ğŸ’¾ Judgeæ•°æ®å·²ä¿å­˜åˆ°: {judge_output_path}")
    
    # ç»Ÿè®¡ç»“æœ
    print(f"\n{'='*60}")
    print("ğŸ“Š æ··åˆåˆ†æå™¨ç»“æœç»Ÿè®¡")
    print(f"{'='*60}")
    
    total_papers = len(all_results)
    total_figures = sum(r["total_figures"] for r in all_results)
    total_diagrams = sum(r["diagram_figures"] for r in all_results)
    successful_analyses = 0
    valid_training_data = 0
    invalid_training_data = 0
    
    for result in all_results:
        paper_name = result["paper_name"]
        total_figs = result["total_figures"]
        diagram_figs = result["diagram_figures"]
        successful = len(result["results"])
        
        # ç»Ÿè®¡æ•°æ®è´¨é‡
        valid_count = sum(1 for r in result["results"] if r.get("training_data", {}).get("data_quality") == "valid")
        invalid_count = sum(1 for r in result["results"] if r.get("training_data", {}).get("data_quality") == "invalid")
        
        print(f"ğŸ“š {paper_name}:")
        print(f"   ğŸ“Š æ€»å›¾ç‰‡æ•°: {total_figs}")
        print(f"   ğŸ¤– GPTè¯†åˆ«diagram: {diagram_figs}")
        print(f"   âœ… æˆåŠŸåˆ†æ: {successful}/{diagram_figs}")
        print(f"   âœ… æœ‰æ•ˆè®­ç»ƒæ•°æ®: {valid_count}")
        print(f"   âŒ æ— æ•ˆè®­ç»ƒæ•°æ®: {invalid_count}")
        print()
        
        successful_analyses += successful
        valid_training_data += valid_count
        invalid_training_data += invalid_count
    
    print(f"ğŸ¯ æ€»ä½“ç»Ÿè®¡:")
    print(f"   ğŸ“š æµ‹è¯•è®ºæ–‡æ•°: {total_papers}")
    print(f"   ğŸ“Š æ€»å›¾ç‰‡æ•°: {total_figures}")
    print(f"   ğŸ¤– GPTè¯†åˆ«diagram: {total_diagrams}")
    print(f"   âœ… æˆåŠŸåˆ†æ: {successful_analyses}/{total_diagrams}")
    print(f"   ğŸ“ˆ æˆåŠŸç‡: {successful_analyses/total_diagrams*100:.1f}%" if total_diagrams > 0 else "   ğŸ“ˆ æˆåŠŸç‡: N/A")
    print(f"   ğŸ¯ è¯†åˆ«ç‡: {total_diagrams/total_figures*100:.1f}%" if total_figures > 0 else "   ğŸ¯ è¯†åˆ«ç‡: N/A")
    print(f"   âœ… æœ‰æ•ˆè®­ç»ƒæ•°æ®: {valid_training_data}")
    print(f"   âŒ æ— æ•ˆè®­ç»ƒæ•°æ®: {invalid_training_data}")
    print(f"   ğŸ“Š æ•°æ®è´¨é‡ç‡: {valid_training_data/(valid_training_data+invalid_training_data)*100:.1f}%" if (valid_training_data+invalid_training_data) > 0 else "   ğŸ“Š æ•°æ®è´¨é‡ç‡: N/A")
    
    print(f"\nğŸ’¾ å®Œæ•´è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {training_output_path}")
    print(f"âœ… æœ€ç»ˆä½¿ç”¨çš„APIæº: {reasoner.api_source.value}")


if __name__ == "__main__":
    import json
    main()
