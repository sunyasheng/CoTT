#!/usr/bin/env python3
"""
Azureç®€å•å¹¶è¡Œå¤„ç†å™¨ - ç›´æ¥å¤ç”¨azure_diagram_reasoner.pyçš„é€»è¾‘

åŠŸèƒ½ï¼š
1. æ‰«ææŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰markdownæ–‡ä»¶
2. ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†æ¯ä¸ªæ–‡ä»¶
3. ç›´æ¥è°ƒç”¨azure_diagram_reasoner.pyä¸­çš„test_smart_markdown_paperå‡½æ•°
4. åˆå¹¶æ‰€æœ‰ç»“æœ

ä½¿ç”¨æ–¹æ³•ï¼š
python azure_parallel_processor.py --input_dir /path/to/markdown/files --output_dir /path/to/output --workers 4
"""

import os
import sys
import json
import argparse
import logging
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any
import time
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from azure_diagram_reasoner import test_smart_markdown_paper
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥azure_diagram_reasoner: {e}")
    sys.exit(1)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('azure_simple_parallel_processor.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class AzureSimpleParallelProcessor:
    """Azureç®€å•å¹¶è¡Œå¤„ç†å™¨ - ç›´æ¥å¤ç”¨azure_diagram_reasoneré€»è¾‘"""
    
    def __init__(self, max_workers: int = 4, skip_existing: bool = True):
        self.max_workers = max_workers
        self.skip_existing = skip_existing
        
    def find_markdown_files(self, input_dir: str) -> List[Path]:
        """æ‰«æç›®å½•ä¸‹çš„æ‰€æœ‰markdownæ–‡ä»¶"""
        input_path = Path(input_dir)
        if not input_path.exists():
            logger.error(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
            return []
        
        # æŸ¥æ‰¾æ‰€æœ‰.mdæ–‡ä»¶
        markdown_files = []
        for pattern in ["**/*.md", "**/*.markdown"]:
            markdown_files.extend(input_path.glob(pattern))
        
        # è¿‡æ»¤æ‰éšè—æ–‡ä»¶å’Œä¸´æ—¶æ–‡ä»¶
        markdown_files = [f for f in markdown_files if not f.name.startswith('.')]
        
        logger.info(f"ğŸ“ åœ¨ {input_dir} ä¸­æ‰¾åˆ° {len(markdown_files)} ä¸ªmarkdownæ–‡ä»¶")
        return markdown_files
    
    def check_existing_data(self, output_dir: Path, file_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è®­ç»ƒæ•°æ®æ–‡ä»¶"""
        file_output_dir = output_dir / file_name
        training_file = file_output_dir / f"{file_name}_training_data.json"
        judge_file = file_output_dir / f"{file_name}_judge_data.json"
        
        # æ£€æŸ¥ä¸¤ä¸ªæ–‡ä»¶æ˜¯å¦éƒ½å­˜åœ¨ä¸”ä¸ä¸ºç©º
        if training_file.exists() and judge_file.exists():
            try:
                # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦æœ‰æ•ˆ
                with open(training_file, 'r', encoding='utf-8') as f:
                    training_data = json.load(f)
                with open(judge_file, 'r', encoding='utf-8') as f:
                    judge_data = json.load(f)
                
                # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
                if training_data and judge_data:
                    logger.info(f"âœ… å‘ç°å·²å­˜åœ¨çš„è®­ç»ƒæ•°æ®: {file_name}")
                    return True
                else:
                    logger.info(f"âš ï¸ è®­ç»ƒæ•°æ®æ–‡ä»¶å­˜åœ¨ä½†ä¸ºç©º: {file_name}")
                    return False
            except (json.JSONDecodeError, Exception) as e:
                logger.warning(f"âš ï¸ è®­ç»ƒæ•°æ®æ–‡ä»¶æŸå: {file_name}, é”™è¯¯: {e}")
                return False
        
        return False

    def load_existing_data(self, output_dir: Path, file_name: str) -> Dict[str, Any]:
        """åŠ è½½å·²å­˜åœ¨çš„è®­ç»ƒæ•°æ®"""
        file_output_dir = output_dir / file_name
        training_file = file_output_dir / f"{file_name}_training_data.json"
        judge_file = file_output_dir / f"{file_name}_judge_data.json"
        
        try:
            with open(training_file, 'r', encoding='utf-8') as f:
                training_data = json.load(f)
            with open(judge_file, 'r', encoding='utf-8') as f:
                judge_data = json.load(f)
            
            return {
                "training_data": training_data,
                "judge_data": judge_data,
                "statistics": {
                    "total_figures": len(training_data),  # ä¼°ç®—
                    "diagram_figures": len(training_data),
                    "processed_results": len(training_data)
                }
            }
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å·²å­˜åœ¨æ•°æ®å¤±è´¥: {file_name}, é”™è¯¯: {e}")
            return None

    def process_single_file(self, markdown_file: Path, output_dir: Path) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªmarkdownæ–‡ä»¶ - ç›´æ¥è°ƒç”¨azure_diagram_reasonerå‡½æ•°"""
        start_time = time.time()
        file_result = {
            "file_path": str(markdown_file),
            "file_name": markdown_file.name,
            "status": "processing",
            "start_time": datetime.now().isoformat(),
            "error": None,
            "training_data": [],
            "judge_data": [],
            "statistics": {}
        }
        
        try:
            logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†: {markdown_file.name}")
            
            # åˆ›å»ºæ¯ä¸ªæ–‡ä»¶çš„è¾“å‡ºç›®å½•
            file_output_dir = output_dir / markdown_file.stem
            file_output_dir.mkdir(parents=True, exist_ok=True)
            
            # æ³¨æ„ï¼šè·³è¿‡çš„æ–‡ä»¶å·²ç»åœ¨ä¸»æµç¨‹ä¸­å¤„ç†ï¼Œè¿™é‡Œåªå¤„ç†éœ€è¦é‡æ–°ç”Ÿæˆçš„æ–‡ä»¶
            
            # ç›´æ¥è°ƒç”¨azure_diagram_reasonerä¸­çš„test_smart_markdown_paperå‡½æ•°
            # è¿™ä¸ªå‡½æ•°å·²ç»åŒ…å«äº†å®Œæ•´çš„å¤„ç†é€»è¾‘
            paper_dir = markdown_file.parent.parent  # å›åˆ°è®ºæ–‡æ ¹ç›®å½•
            paper_name = markdown_file.stem
            
            logger.info(f"ğŸ“ è®ºæ–‡ç›®å½•: {paper_dir}")
            logger.info(f"ğŸ“„ è®ºæ–‡åç§°: {paper_name}")
            
            result = test_smart_markdown_paper(paper_dir, paper_name)
            
            if result and "results" in result and result["results"]:
                # åˆå¹¶æ‰€æœ‰resultsä¸­çš„training_dataå’Œjudge_data
                all_training_data = []
                all_judge_data = []
                
                for res in result["results"]:
                    if "training_data" in res:
                        all_training_data.append(res["training_data"])
                    if "judge_data" in res:
                        all_judge_data.append(res["judge_data"])
                
                file_result["training_data"] = all_training_data
                file_result["judge_data"] = all_judge_data
                file_result["statistics"] = {
                    "total_figures": result.get("total_figures", 0),
                    "diagram_figures": result.get("diagram_figures", 0),
                    "processed_results": len(result.get("results", []))
                }
                file_result["status"] = "completed"
                
                # ä¿å­˜å•ä¸ªæ–‡ä»¶çš„ç»“æœ
                self.save_file_results(all_training_data, all_judge_data, file_output_dir, markdown_file.stem)
                
                logger.info(f"âœ… å®Œæˆå¤„ç†: {markdown_file.name}")
            else:
                file_result["status"] = "failed"
                file_result["error"] = "å¤„ç†è¿”å›ç©ºç»“æœæˆ–æ²¡æœ‰results"
                logger.error(f"âŒ å¤„ç†å¤±è´¥: {markdown_file.name}")
                
        except Exception as e:
            file_result["status"] = "failed"
            file_result["error"] = str(e)
            logger.error(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {markdown_file.name}: {e}")
        
        file_result["end_time"] = datetime.now().isoformat()
        file_result["processing_time"] = time.time() - start_time
        
        return file_result
    
    def save_file_results(self, training_data: List[Dict], judge_data: List[Dict], 
                         output_dir: Path, file_name: str):
        """ä¿å­˜å•ä¸ªæ–‡ä»¶çš„ç»“æœ"""
        # ä¿å­˜è®­ç»ƒæ•°æ®
        training_file = output_dir / f"{file_name}_training_data.json"
        with open(training_file, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜judgeæ•°æ®
        judge_file = output_dir / f"{file_name}_judge_data.json"
        with open(judge_file, 'w', encoding='utf-8') as f:
            json.dump(judge_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_dir}")
    
    def scan_existing_files(self, markdown_files: List[Path], output_dir: Path) -> tuple:
        """æ‰«æå·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œè¿”å›éœ€è¦å¤„ç†å’Œè·³è¿‡çš„æ–‡ä»¶åˆ—è¡¨"""
        if not self.skip_existing:
            return markdown_files, []
        
        files_to_process = []
        files_to_skip = []
        
        logger.info(f"ğŸ” æ‰«æå·²å­˜åœ¨çš„è®­ç»ƒæ•°æ®æ–‡ä»¶...")
        
        for file in markdown_files:
            if self.check_existing_data(output_dir, file.stem):
                files_to_skip.append(file)
            else:
                files_to_process.append(file)
        
        logger.info(f"ğŸ“Š æ‰«æç»“æœ:")
        logger.info(f"   â­ï¸ å°†è·³è¿‡ {len(files_to_skip)} ä¸ªå·²å­˜åœ¨æ•°æ®çš„æ–‡ä»¶")
        logger.info(f"   ğŸ”„ å°†å¤„ç† {len(files_to_process)} ä¸ªæ–°æ–‡ä»¶")
        
        if files_to_skip:
            logger.info(f"   ğŸ“‹ è·³è¿‡çš„æ–‡ä»¶åˆ—è¡¨:")
            for file in files_to_skip[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                logger.info(f"      - {file.name}")
            if len(files_to_skip) > 10:
                logger.info(f"      ... è¿˜æœ‰ {len(files_to_skip) - 10} ä¸ªæ–‡ä»¶")
        
        return files_to_process, files_to_skip

    def process_parallel(self, input_dir: str, output_dir: str) -> Dict[str, Any]:
        """å¹¶è¡Œå¤„ç†æ‰€æœ‰markdownæ–‡ä»¶"""
        start_time = time.time()
        
        # æŸ¥æ‰¾æ‰€æœ‰markdownæ–‡ä»¶
        markdown_files = self.find_markdown_files(input_dir)
        if not markdown_files:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°markdownæ–‡ä»¶"}
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # æ‰«æå·²å­˜åœ¨çš„æ–‡ä»¶
        files_to_process, files_to_skip = self.scan_existing_files(markdown_files, output_path)
        
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {len(files_to_process)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ {self.max_workers} ä¸ªçº¿ç¨‹")
        
        # å¹¶è¡Œå¤„ç†
        results = []
        failed_files = []
        skipped_files = []
        
        # å…ˆå¤„ç†è·³è¿‡çš„æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½å·²å­˜åœ¨çš„æ•°æ®
        logger.info(f"ğŸ“¥ åŠ è½½ {len(files_to_skip)} ä¸ªå·²å­˜åœ¨æ–‡ä»¶çš„æ•°æ®...")
        for i, file in enumerate(files_to_skip, 1):
            if i % 1000 == 0:  # æ¯1000ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                logger.info(f"   ğŸ“¥ å·²åŠ è½½ {i}/{len(files_to_skip)} ä¸ªè·³è¿‡æ–‡ä»¶")
            
            existing_data = self.load_existing_data(output_path, file.stem)
            if existing_data:
                skipped_result = {
                    "file_path": str(file),
                    "file_name": file.name,
                    "status": "skipped",
                    "start_time": datetime.now().isoformat(),
                    "end_time": datetime.now().isoformat(),
                    "processing_time": 0.0,
                    "error": None,
                    "training_data": existing_data["training_data"],
                    "judge_data": existing_data["judge_data"],
                    "statistics": existing_data["statistics"]
                }
                skipped_files.append(skipped_result)
                results.append(skipped_result)
        
        if files_to_skip:
            logger.info(f"âœ… å®ŒæˆåŠ è½½ {len(skipped_files)} ä¸ªè·³è¿‡æ–‡ä»¶çš„æ•°æ®")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # åªæäº¤éœ€è¦å¤„ç†çš„ä»»åŠ¡
            future_to_file = {
                executor.submit(self.process_single_file, file, output_path): file 
                for file in files_to_process
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_file):
                file = future_to_file[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    if result["status"] == "failed":
                        failed_files.append(result)
                        logger.error(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {file.name}")
                    elif result["status"] == "skipped":
                        skipped_files.append(result)
                        logger.info(f"â­ï¸ æ–‡ä»¶å·²è·³è¿‡: {file.name}")
                    else:
                        logger.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {file.name}")
                        
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºç°å¼‚å¸¸ {file.name}: {e}")
                    failed_files.append({
                        "file_path": str(file),
                        "file_name": file.name,
                        "status": "failed",
                        "error": str(e)
                    })
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_training_data = []
        all_judge_data = []
        
        for result in results:
            if result["status"] in ["completed", "skipped"]:
                all_training_data.extend(result.get("training_data", []))
                all_judge_data.extend(result.get("judge_data", []))
        
        # ä¿å­˜åˆå¹¶ç»“æœ
        self.save_combined_results(all_training_data, all_judge_data, results, output_path)
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        total_time = time.time() - start_time
        successful_files = len([r for r in results if r["status"] == "completed"])
        statistics = {
            "total_files": len(markdown_files),
            "files_to_process": len(files_to_process),
            "files_to_skip": len(files_to_skip),
            "successful_files": successful_files,
            "skipped_files": len(skipped_files),
            "failed_files": len(failed_files),
            "total_training_items": len(all_training_data),
            "total_judge_items": len(all_judge_data),
            "total_processing_time": total_time,
            "average_time_per_file": total_time / len(files_to_process) if files_to_process else 0,
            "skip_existing_enabled": self.skip_existing
        }
        
        logger.info(f"ğŸ‰ å¹¶è¡Œå¤„ç†å®Œæˆï¼")
        logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {statistics}")
        
        return {
            "statistics": statistics,
            "results": results,
            "failed_files": failed_files,
            "skipped_files": skipped_files,
            "all_training_data": all_training_data,
            "all_judge_data": all_judge_data
        }
    
    def save_combined_results(self, training_data: List[Dict], judge_data: List[Dict], 
                            results: List[Dict], output_dir: Path):
        """ä¿å­˜åˆå¹¶çš„ç»“æœ"""
        # ä¿å­˜åˆå¹¶çš„è®­ç»ƒæ•°æ®
        training_file = output_dir / "azure_combined_training_data.json"
        with open(training_file, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜åˆå¹¶çš„judgeæ•°æ®
        judge_file = output_dir / "azure_combined_judge_data.json"
        with open(judge_file, 'w', encoding='utf-8') as f:
            json.dump(judge_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å¤„ç†æŠ¥å‘Š
        report_file = output_dir / "azure_processing_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ Azureåˆå¹¶ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Azureç®€å•å¹¶è¡Œå¤„ç†markdownæ–‡ä»¶ä¸­çš„å›¾è¡¨")
    parser.add_argument("--input_dir", "-i", required=True, help="è¾“å…¥markdownæ–‡ä»¶ç›®å½•")
    parser.add_argument("--output_dir", "-o", required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--workers", "-w", type=int, default=4, help="å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°")
    parser.add_argument("--no-skip-existing", action="store_true", 
                       help="ç¦ç”¨è·³è¿‡å·²å­˜åœ¨è®­ç»ƒæ•°æ®çš„åŠŸèƒ½ï¼Œå¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰æ–‡ä»¶")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = AzureSimpleParallelProcessor(max_workers=args.workers, skip_existing=not args.no_skip_existing)
    
    # å¼€å§‹å¤„ç†
    result = processor.process_parallel(args.input_dir, args.output_dir)
    
    if "error" in result:
        logger.error(f"âŒ å¤„ç†å¤±è´¥: {result['error']}")
        sys.exit(1)
    else:
        logger.info("ğŸ‰ æ‰€æœ‰å¤„ç†å®Œæˆï¼")


if __name__ == "__main__":
    main()
