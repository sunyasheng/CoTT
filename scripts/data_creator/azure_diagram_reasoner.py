#!/usr/bin/env python3
"""
Smart Diagram Figure Reasoner - ä½¿ç”¨GPTæ™ºèƒ½åˆ¤æ–­å›¾ç‰‡ç±»å‹

åŠŸèƒ½ï¼š
1. æå–æ‰€æœ‰å›¾ç‰‡çš„caption
2. ä½¿ç”¨GPTåˆ¤æ–­å“ªäº›æ˜¯diagram
3. åªåˆ†æè¢«GPTè¯†åˆ«ä¸ºdiagramçš„å›¾ç‰‡
4. ç”Ÿæˆç»˜å›¾æŒ‡ä»¤

è¿™ç§æ–¹æ³•æ¯”å…³é”®è¯åŒ¹é…æ›´å‡†ç¡®å’Œç¨³å®š
"""

import os
import re
import json
import requests
import base64
import time
from pathlib import Path
from typing import Dict, List

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, AzureOpenAIEmbeddings
from prompts_template.prompt_manager import prompt_manager
from langchain.schema import Document
HAS_LANGCHAIN = True

from dotenv import load_dotenv
HAS_DOTENV = True

def load_env_vars():
    """åŠ è½½ç¯å¢ƒå˜é‡"""
    # å°è¯•ä»å¤šä¸ªä½ç½®åŠ è½½ .env æ–‡ä»¶
    env_paths = [
        Path(__file__).parent / ".env",
        Path(__file__).parent.parent / ".env",
        Path(__file__).parent.parent.parent / "CoTT" / ".env",
        Path(__file__).parent.parent.parent / "CoTT" / ".env_old",
        Path("/home/suny0a/Proj/CoTT/.env"),  # Linux ç»å¯¹è·¯å¾„
        Path("/Users/suny0a/Proj/MM-Reasoning/CoTT/.env"),  # macOS ç»å¯¹è·¯å¾„
    ]
    
    if HAS_DOTENV:
        for env_path in env_paths:
            if env_path.exists():
                print(f"ğŸ“„ åŠ è½½ç¯å¢ƒå˜é‡: {env_path}")
                load_dotenv(env_path)
                return True
    else:
        # ç®€å•çš„ç¯å¢ƒå˜é‡åŠ è½½
        for env_path in env_paths:
            if env_path.exists():
                print(f"ğŸ“„ ä»æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡: {env_path}")
                with open(env_path, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#') and '=' in line:
                            key, value = line.split('=', 1)
                            os.environ[key] = value
                
                # è®¾ç½®é»˜è®¤çš„ Azure OpenAI é…ç½®
                if not os.getenv("AZURE_OPENAI_ENDPOINT"):
                    os.environ["AZURE_OPENAI_ENDPOINT"] = "https://linjl-ma65uv6u-eastus2.cognitiveservices.azure.com/"
                if not os.getenv("AZURE_OPENAI_DEPLOYMENT"):
                    os.environ["AZURE_OPENAI_DEPLOYMENT"] = "gpt-4o"
                if not os.getenv("AZURE_OPENAI_API_VERSION"):
                    os.environ["AZURE_OPENAI_API_VERSION"] = "2024-02-15-preview"
                print(f"   âœ… è®¾ç½®é»˜è®¤ Azure OpenAI é…ç½®")
                return True
    
    print("âš ï¸ æœªæ‰¾åˆ°ç¯å¢ƒå˜é‡æ–‡ä»¶")
    return False

# åŠ è½½ç¯å¢ƒå˜é‡
load_env_vars()

# å…¨å±€å˜é‡ç”¨äºæ§åˆ¶APIè¯·æ±‚é—´éš”
last_api_call_time = 0
api_call_interval = 1.0  # 1ç§’é—´éš”ï¼Œé¿å…rate limit

def make_api_request_with_retry(url: str, headers: Dict, payload: Dict, max_retries: int = 3, delay: float = 2.0, timeout: int = 180) -> Dict:
    """å¸¦é‡è¯•æœºåˆ¶çš„APIè¯·æ±‚ - ä¸hybridç‰ˆæœ¬ä¿æŒä¸€è‡´"""
    global last_api_call_time, api_call_interval
    
    for attempt in range(max_retries):
        try:
            # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œé¿å…rate limit
            current_time = time.time()
            time_since_last_call = current_time - last_api_call_time
            if time_since_last_call < api_call_interval:
                sleep_time = api_call_interval - time_since_last_call
                print(f"   â³ ç­‰å¾… {sleep_time:.1f} ç§’é¿å…rate limit...")
                time.sleep(sleep_time)
            
            response = requests.post(url, headers=headers, json=payload, timeout=timeout)
            last_api_call_time = time.time()  # æ›´æ–°æœ€åè°ƒç”¨æ—¶é—´
            
            if response.status_code == 401:
                print(f"   âš ï¸ 401é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    print(f"   â³ ç­‰å¾… {delay} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    delay *= 1.5  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    response.raise_for_status()
            elif response.status_code == 429:
                print(f"   âš ï¸ 429 Rate Limit (å°è¯• {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    wait_time = delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    print(f"   â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    response.raise_for_status()
            elif response.status_code == 408:
                print(f"   âš ï¸ 408 Request Timeout (å°è¯• {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    wait_time = delay * (1.5 ** attempt)  # é€‚ä¸­çš„é€€é¿
                    print(f"   â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    response.raise_for_status()
            else:
                response.raise_for_status()
            
            return response.json()
            
        except requests.exceptions.RequestException as e:
            print(f"   âŒ APIè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                print(f"   â³ ç­‰å¾… {delay} ç§’åé‡è¯•...")
                time.sleep(delay)
                delay *= 1.5
            else:
                raise e
    
    return {"error": f"API request failed after {max_retries} attempts"}

def extract_json_from_markdown(content: str) -> Dict:
    """ä»markdownå†…å®¹ä¸­æå–JSON"""
    try:
        # æŸ¥æ‰¾```jsonå’Œ```ä¹‹é—´çš„å†…å®¹
        json_match = re.search(r'```json\s*\n(.*?)\n```', content, re.DOTALL)
        if json_match:
            json_content = json_match.group(1)
            return json.loads(json_content)
        
        # å°è¯•æŸ¥æ‰¾```å’Œ```ä¹‹é—´çš„å†…å®¹ï¼ˆæ²¡æœ‰jsonæ ‡è®°ï¼‰
        code_match = re.search(r'```\s*\n(.*?)\n```', content, re.DOTALL)
        if code_match:
            json_content = code_match.group(1)
            return json.loads(json_content)
        
        # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥è§£æ
        return json.loads(content)
        
    except json.JSONDecodeError:
        return {}

def extract_all_figures_from_markdown(markdown_content: str) -> List[Dict]:
    """æå–æ‰€æœ‰å›¾ç‰‡ä¿¡æ¯ï¼Œä¸åšä»»ä½•è¿‡æ»¤"""
    figures = []
    seen_images = set()  # é¿å…é‡å¤æå–
    
    # æ‰¾åˆ°é™„å½•å¼€å§‹çš„ä½ç½®ï¼ˆæ’é™¤é™„å½•å†…å®¹ï¼‰
    lines = markdown_content.split('\n')
    appendix_start_idx = len(lines)  # é»˜è®¤æ²¡æœ‰é™„å½•
    for i, line in enumerate(lines):
        line_lower = line.lower().strip()
        # æ£€æŸ¥æ˜¯å¦æ˜¯é™„å½•æ ‡é¢˜
        if (line_lower.startswith('# appendix') or 
            line_lower.startswith('## appendix') or
            line_lower.startswith('### appendix') or
            line_lower.startswith('# supplementary') or
            line_lower.startswith('## supplementary') or
            line_lower.startswith('### supplementary') or
            'appendix' in line_lower and line_lower.startswith('#')):
            appendix_start_idx = i
            break
    
    # åªå¤„ç†æ­£æ–‡éƒ¨åˆ†ï¼ˆæ’é™¤é™„å½•ï¼‰
    main_content = '\n'.join(lines[:appendix_start_idx])
    
    # æŸ¥æ‰¾markdownæ ¼å¼çš„å›¾ç‰‡å¼•ç”¨: ![](path) æˆ– ![alt](path)
    image_pattern = r'!\[([^\]]*)\]\(([^)]+\.(?:jpg|jpeg|png|gif|bmp|svg))\)'
    image_matches = re.findall(image_pattern, main_content, re.IGNORECASE)
    
    # å¤„ç†æ¯ä¸ªå›¾ç‰‡å¼•ç”¨
    for alt_text, image_path in image_matches:
        # é¿å…é‡å¤å¤„ç†
        if image_path in seen_images:
            continue
        seen_images.add(image_path)
        
        # æŸ¥æ‰¾è¿™ä¸ªå›¾ç‰‡åé¢çš„caption
        caption = ""
        image_pos = main_content.find(f"![{alt_text}]({image_path})")
        if image_pos != -1:
            # æŸ¥æ‰¾å›¾ç‰‡åçš„æ–‡æœ¬ä½œä¸ºcaption
            after_image = main_content[image_pos + len(f"![{alt_text}]({image_path})"):]
            # æŸ¥æ‰¾ä¸‹ä¸€è¡Œæˆ–æ®µè½ä½œä¸ºcaption
            lines_after = after_image.split('\n')
            for line in lines_after:
                line = line.strip()
                if line and not line.startswith('![') and not line.startswith('#'):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯Figureå¼€å¤´çš„caption
                    if line.lower().startswith('figure'):
                        caption = line
                        break
                    # æˆ–è€…å–ç¬¬ä¸€è¡Œéç©ºæ–‡æœ¬ä½œä¸ºcaption
                    elif not caption:
                        caption = line
                    # å¦‚æœé‡åˆ°ä¸‹ä¸€ä¸ªå›¾ç‰‡æˆ–æ ‡é¢˜ï¼Œåœæ­¢
                    if line.startswith('![') or line.startswith('#'):
                        break
        
        figures.append({
            'id': f"figure_{len(figures) + 1}",
            'src': image_path,
            'caption': caption or alt_text or f"Figure {len(figures) + 1}",
            'alt_text': alt_text
        })
    
    return figures


def classify_figures_with_gpt(figures: List[Dict], paper_title: str = "") -> List[Dict]:
    """ä½¿ç”¨GPTåˆ¤æ–­å“ªäº›å›¾ç‰‡æ˜¯diagram"""
    
    # Azure OpenAI é…ç½®
    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "https://linjl-ma65uv6u-eastus2.cognitiveservices.azure.com/")
    api_key = os.getenv("AZURE_OPENAI_API_KEY", "")
    deployment = "gpt-4o"  # ä½¿ç”¨gpt-4oè¿›è¡Œåˆ†ç±»
    api_version = "2024-02-15-preview"
    
    if not api_key:
        print("âŒ Azure OpenAI API key not found")
        return []
    
    # æ„å»ºè¯·æ±‚URL
    url = f"{endpoint}openai/deployments/{deployment}/chat/completions?api-version={api_version}"
    headers = {"Content-Type": "application/json", "api-key": api_key}
    
    # æ„å»ºæ‰€æœ‰å›¾ç‰‡çš„captionä¿¡æ¯
    figure_info = []
    for i, fig in enumerate(figures, 1):
        figure_info.append(f"Figure {i}: {fig['caption']}")
    
    figures_text = "\n".join(figure_info)
    
    # ä½¿ç”¨promptæ¨¡æ¿
    prompt = prompt_manager.get_figure_classification_prompt(paper_title, figures_text)

    payload = {
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "max_tokens": 1000,
        "temperature": 0.1
    }
    
    try:
        data = make_api_request_with_retry(url, headers, payload, max_retries=3, delay=2.0, timeout=60)
        content = data.get("choices", [{}])[0].get("message", {}).get("content", "{}")
        
        # å°è¯•è§£æJSON
        try:
            # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
            result = json.loads(content)
        except json.JSONDecodeError:
            # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æå–markdownä»£ç å—ä¸­çš„JSON
            try:
                # æŸ¥æ‰¾```jsonå’Œ```ä¹‹é—´çš„å†…å®¹
                json_match = re.search(r'```json\s*\n(.*?)\n```', content, re.DOTALL)
                if json_match:
                    json_content = json_match.group(1)
                    result = json.loads(json_content)
                else:
                    # å°è¯•æŸ¥æ‰¾```å’Œ```ä¹‹é—´çš„å†…å®¹ï¼ˆæ²¡æœ‰jsonæ ‡è®°ï¼‰
                    code_match = re.search(r'```\s*\n(.*?)\n```', content, re.DOTALL)
                    if code_match:
                        json_content = code_match.group(1)
                        result = json.loads(json_content)
                    else:
                        print(f"   âŒ æ— æ³•æ‰¾åˆ°JSONå†…å®¹: {content}")
                        return []
            except json.JSONDecodeError:
                print(f"   âŒ æ— æ³•è§£æGPTåˆ†ç±»ç»“æœ: {content}")
                return []
        
        # è§£ææˆåŠŸï¼Œæå–ç»“æœ
        diagram_figure_numbers = result.get("diagram_figures", [])
        reasoning = result.get("reasoning", "")
        
        print(f"   ğŸ¤– GPTåˆ†ç±»ç»“æœ: {reasoning}")
        
        # æ ¹æ®GPTçš„åˆ†ç±»ç»“æœç­›é€‰å›¾ç‰‡
        diagram_figures = []
        for i, fig in enumerate(figures, 1):
            if i in diagram_figure_numbers:
                fig['type'] = 'diagram'
                fig['gpt_reasoning'] = reasoning
                diagram_figures.append(fig)
        
        return diagram_figures
            
    except Exception as e:
        print(f"   âŒ GPTåˆ†ç±»å¤±è´¥: {str(e)}")
        return []


class FAISSRetriever:
    """åŸºäºFAISSçš„å¯†é›†å‘é‡æ£€ç´¢å™¨"""
    
    def __init__(self):
        self.vector_store = None
        self.documents = []
        self.embeddings = None
        
    def _get_embeddings(self):
        """è·å–embeddingæ¨¡å‹"""
        if self.embeddings is not None:
            return self.embeddings
            
        # æ£€æŸ¥Azure OpenAIé…ç½®
        if os.getenv("AZURE_OPENAI_ENDPOINT") and os.getenv("AZURE_OPENAI_API_KEY"):
            endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
            api_key = os.getenv("AZURE_OPENAI_API_KEY")
            api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-06-01")
            
            self.embeddings = AzureOpenAIEmbeddings(
                azure_endpoint=endpoint,
                openai_api_version=api_version,
                openai_api_key=api_key,
                model="text-embedding-3-large"
            )
            print("   ğŸ”— ä½¿ç”¨ Azure OpenAI Embeddings")
        else:
            # ä½¿ç”¨OpenAI API
            self.embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
            print("   ğŸ”— ä½¿ç”¨ OpenAI Embeddings")
        
        return self.embeddings
    
    def fit(self, documents: List[str]):
        """æ„å»ºFAISSç´¢å¼•"""
        if not HAS_LANGCHAIN:
            raise ImportError("éœ€è¦å®‰è£…langchainæ¥ä½¿ç”¨FAISSæ£€ç´¢")
        
        self.documents = documents
        
        # åˆ›å»ºDocumentå¯¹è±¡
        docs = [Document(page_content=doc) for doc in documents]
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = text_splitter.split_documents(docs)
        
        # è·å–embeddingså¹¶æ„å»ºFAISSç´¢å¼•
        embeddings = self._get_embeddings()
        self.vector_store = FAISS.from_documents(chunks, embeddings)
        
        print(f"   ğŸ“Š FAISSç´¢å¼•æ„å»ºå®Œæˆ: {len(chunks)} ä¸ªchunks")
    
    def search(self, query: str, top_k: int = 3) -> List[tuple]:
        """æœç´¢æœ€ç›¸å…³çš„æ–‡æ¡£"""
        if self.vector_store is None:
            return []
        
        # ä½¿ç”¨FAISSæ£€ç´¢
        retriever = self.vector_store.as_retriever(search_kwargs={"k": top_k})
        try:
            # ä½¿ç”¨æ–°çš„invokeæ–¹æ³•
            docs = retriever.invoke(query)
        except AttributeError:
            # å›é€€åˆ°æ—§æ–¹æ³•
            docs = retriever.get_relevant_documents(query)
        
        # è¿”å›ç»“æœ
        results = []
        for i, doc in enumerate(docs):
            # æ‰¾åˆ°åŸå§‹æ–‡æ¡£çš„ç´¢å¼•
            original_idx = self._find_original_doc_index(doc.page_content)
            if original_idx != -1:
                results.append((original_idx, 1.0 - i * 0.1))  # ç®€å•çš„åˆ†æ•°è®¡ç®—
        
        return results
    
    def _find_original_doc_index(self, content: str) -> int:
        """æ‰¾åˆ°chunkå¯¹åº”çš„åŸå§‹æ–‡æ¡£ç´¢å¼•"""
        for i, doc in enumerate(self.documents):
            if content[:100] in doc:  # ä½¿ç”¨å‰100ä¸ªå­—ç¬¦åŒ¹é…
                return i
        return -1


def get_semantic_context_for_figure(markdown_content: str, caption: str) -> str:
    """ä½¿ç”¨FAISSæ£€ç´¢æ‰¾åˆ°ä¸å›¾ç‰‡ç›¸å…³çš„ä¸Šä¸‹æ–‡å†…å®¹"""
    if not HAS_LANGCHAIN:
        print("   âŒ langchain æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨FAISSæ£€ç´¢")
        return ""
    
    lines = markdown_content.split('\n')
    
    # æ‰¾åˆ°é™„å½•å¼€å§‹çš„ä½ç½®ï¼ˆæ’é™¤é™„å½•å†…å®¹ï¼‰
    appendix_start_idx = len(lines)  # é»˜è®¤æ²¡æœ‰é™„å½•
    for i, line in enumerate(lines):
        line_lower = line.lower().strip()
        # æ£€æŸ¥æ˜¯å¦æ˜¯é™„å½•æ ‡é¢˜
        if (line_lower.startswith('# appendix') or 
            line_lower.startswith('## appendix') or
            line_lower.startswith('### appendix') or
            line_lower.startswith('# supplementary') or
            line_lower.startswith('## supplementary') or
            line_lower.startswith('### supplementary') or
            'appendix' in line_lower and line_lower.startswith('#')):
            appendix_start_idx = i
            break
    
    # å°†æ–‡æ¡£åˆ†å‰²æˆæ®µè½ï¼ˆåªè€ƒè™‘æ­£æ–‡éƒ¨åˆ†ï¼Œæ’é™¤é™„å½•ï¼‰
    paragraphs = []
    current_paragraph = []
    
    for i, line in enumerate(lines[:appendix_start_idx]):  # åªå¤„ç†æ­£æ–‡éƒ¨åˆ†
        line = line.strip()
        if not line:
            if current_paragraph:
                paragraphs.append(' '.join(current_paragraph))
                current_paragraph = []
        elif not line.startswith('![') and not line.startswith('#'):
            current_paragraph.append(line)
    
    # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
    if current_paragraph:
        paragraphs.append(' '.join(current_paragraph))
    
    # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ®µè½
    paragraphs = [text for text in paragraphs if len(text) > 50]
    
    if not paragraphs:
        return ""
    
    # ä½¿ç”¨FAISSæ£€ç´¢
    try:
        print("   ğŸ” ä½¿ç”¨FAISSå¯†é›†æ£€ç´¢...")
        retriever = FAISSRetriever()
        retriever.fit(paragraphs)
        
        # æ„å»ºæŸ¥è¯¢ï¼šä½¿ç”¨captionä½œä¸ºæŸ¥è¯¢ï¼Œæ‰©å±•ç›¸å…³è¯æ±‡
        query = caption
        if any(keyword in caption.lower() for keyword in ['diagram', 'architecture', 'framework', 'pipeline', 'overview', 'structure']):
            query += " system design components modules workflow process"
        print(f"   ğŸ” FAISSæŸ¥è¯¢: {query[:100]}...")
        
        # æœç´¢æœ€ç›¸å…³çš„æ®µè½
        results = retriever.search(query, top_k=5)
        
        # æå–ç›¸å…³æ®µè½
        relevant_contexts = []
        for doc_idx, score in results:
            if score > 0:  # åªä¿ç•™æœ‰åˆ†æ•°çš„ç»“æœ
                text = paragraphs[doc_idx]
                relevant_contexts.append(text)
                print(f"   ğŸ“„ æ‰¾åˆ°ç›¸å…³æ®µè½ (åˆ†æ•°: {score:.3f}): {text[:100]}...")
        
        return '\n\n'.join(relevant_contexts)
        
    except Exception as e:
        print(f"   âŒ FAISSæ£€ç´¢å¤±è´¥: {e}")
        return ""


def encode_image(image_path: str) -> str:
    """å°†å›¾ç‰‡ç¼–ç ä¸ºbase64"""
    try:
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    except Exception as e:
        print(f"   âŒ å›¾ç‰‡ç¼–ç å¤±è´¥: {e}")
        return ""


def analyze_diagram_with_gpt4o(image_path: str, caption: str, context: str) -> Dict:
    """ä½¿ç”¨GPT-4oåˆ†ædiagramå›¾ç‰‡å†…å®¹"""
    
    # Azure OpenAI é…ç½®
    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "https://linjl-ma65uv6u-eastus2.cognitiveservices.azure.com/")
    api_key = os.getenv("AZURE_OPENAI_API_KEY", "")
    deployment = "gpt-4o"  # ç›´æ¥ä½¿ç”¨gpt-4oéƒ¨ç½²
    api_version = "2024-02-15-preview"
    
    if not api_key:
        return {"error": "Azure OpenAI API key not found"}
    
    # æ„å»ºè¯·æ±‚URL
    url = f"{endpoint}openai/deployments/{deployment}/chat/completions?api-version={api_version}"
    headers = {"Content-Type": "application/json", "api-key": api_key}
    
    # ç¼–ç å›¾ç‰‡
    base64_image = encode_image(image_path)
    if not base64_image:
        return {"error": "Failed to encode image"}
    
    # ä½¿ç”¨promptæ¨¡æ¿
    prompt = prompt_manager.get_diagram_analysis_prompt(caption, context)

    payload = {
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    }
                ]
            }
        ],
        "max_tokens": 3000,
        "temperature": 0.1
    }
    
    try:
        data = make_api_request_with_retry(url, headers, payload, max_retries=3, delay=3.0, timeout=180)
        content = data.get("choices", [{}])[0].get("message", {}).get("content", "{}")
        
        # å°è¯•è§£æJSON
        try:
            # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
            result = json.loads(content)
            return result
        except json.JSONDecodeError:
            # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æå–markdownä»£ç å—ä¸­çš„JSON
            try:
                # æŸ¥æ‰¾```jsonå’Œ```ä¹‹é—´çš„å†…å®¹
                json_match = re.search(r'```json\s*\n(.*?)\n```', content, re.DOTALL)
                if json_match:
                    json_content = json_match.group(1)
                    result = json.loads(json_content)
                    return result
                else:
                    # å°è¯•æŸ¥æ‰¾```å’Œ```ä¹‹é—´çš„å†…å®¹ï¼ˆæ²¡æœ‰jsonæ ‡è®°ï¼‰
                    code_match = re.search(r'```\s*\n(.*?)\n```', content, re.DOTALL)
                    if code_match:
                        json_content = code_match.group(1)
                        result = json.loads(json_content)
                        return result
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»£ç å—ï¼Œå°è¯•æ¸…ç†å†…å®¹åè§£æ
                        cleaned_content = content.strip()
                        # ç§»é™¤å¯èƒ½çš„markdownæ ¼å¼
                        cleaned_content = re.sub(r'^```.*?\n', '', cleaned_content, flags=re.DOTALL)
                        cleaned_content = re.sub(r'\n```.*?$', '', cleaned_content, flags=re.DOTALL)
                        result = json.loads(cleaned_content)
                        return result
            except json.JSONDecodeError:
                # å¦‚æœè¿˜æ˜¯è§£æå¤±è´¥ï¼Œå°è¯•ä»raw_responseä¸­æå–ï¼ˆè¿™æ˜¯ä¿®å¤çš„å…³é”®ï¼‰
                if "raw_response" in content:
                    # å¦‚æœcontentæœ¬èº«å°±æ˜¯raw_responseæ ¼å¼ï¼Œå°è¯•æå–å…¶ä¸­çš„JSON
                    raw_match = re.search(r'```json\s*\n(.*?)\n```', content, re.DOTALL)
                    if raw_match:
                        json_content = raw_match.group(1)
                        result = json.loads(json_content)
                        return result
                
                # å¦‚æœè¿˜æ˜¯è§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹ç”¨äºè°ƒè¯•
                print(f"   âš ï¸ JSONè§£æå¤±è´¥ï¼ŒåŸå§‹å“åº”: {content[:200]}...")
                return {
                    "raw_response": content,
                    "error": "Failed to parse JSON response",
                    "diagram_analysis_available": False
                }
            
    except Exception as e:
        return {"error": f"API request failed: {str(e)}"}


def extract_flow_from_relationships(relationships: list) -> str:
    """ä»å…³ç³»ä¸­æå–æ•°æ®æµæè¿°"""
    if not relationships:
        return "Sequential processing flow"
    
    # åˆ†æå…³ç³»ä¸­çš„æµå‘
    for rel in relationships:
        if isinstance(rel, dict):
            rel_text = rel.get("description", "")
            rel_type = rel.get("type", "")
        else:
            rel_text = str(rel)
            rel_type = ""
        
        if "->" in rel_text or "data_flow" in rel_type:
            return "Directed flow with clear input-output relationships"
        elif "parallel" in rel_text.lower() or "parallel" in rel_type.lower():
            return "Parallel processing with convergence"
    
    return "Complex interconnected flow"


def extract_dependencies_from_relationships(relationships: list) -> list:
    """ä»å…³ç³»ä¸­æå–ä¾èµ–å…³ç³»"""
    dependencies = []
    for rel in relationships:
        if isinstance(rel, dict):
            # å·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
            dependencies.append({
                "from": rel.get("from", ""),
                "to": rel.get("to", ""),
                "type": rel.get("type", "dependency")
            })
        else:
            # å­—ç¬¦ä¸²æ ¼å¼ï¼Œè§£æ
            rel_str = str(rel)
            if "->" in rel_str:
                parts = rel_str.split("->")
                if len(parts) == 2:
                    dependencies.append({
                        "from": parts[0].strip(),
                        "to": parts[1].strip(),
                        "type": "data_flow"
                    })
    return dependencies


def generate_thinking_with_o3(caption: str, context: str, visual_analysis: str) -> Dict:
    """ä½¿ç”¨GPT-o3ç”Ÿæˆthinkingæè¿°ï¼Œç»¼åˆè§†è§‰åˆ†æå’Œpaper context"""
    
    # Azure OpenAI é…ç½®
    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "https://linjl-ma65uv6u-eastus2.cognitiveservices.azure.com/")
    api_key = os.getenv("AZURE_OPENAI_API_KEY", "")
    deployment = "o3-DR"  # ä½¿ç”¨o3-DRéƒ¨ç½²
    api_version = "2025-01-01-preview"
    
    if not api_key:
        return {"error": "Azure OpenAI API key not found"}
        
    # æ„å»ºURLå’Œheaders
    url = f"{endpoint}/openai/deployments/{deployment}/chat/completions?api-version={api_version}"
    headers = {
        "Content-Type": "application/json",
        "api-key": api_key
    }
    
    # ä½¿ç”¨prompt_managerè·å–prompt
    prompt = prompt_manager.get_thinking_generation_prompt(caption, context, visual_analysis)
    
    payload = {
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "max_completion_tokens": 4000
    }
    
    try:
        data = make_api_request_with_retry(url, headers, payload, max_retries=3, delay=2.0, timeout=120)
        content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
        
        return {
            "summary": content,
            "success": True
        }
            
    except Exception as e:
        return {"error": f"API request failed: {str(e)}"}


def generate_diagram_description_with_o3(caption: str, context: str) -> Dict:
    """ä½¿ç”¨GPT-o3åŒæ—¶ç”Ÿæˆé•¿çŸ­ä¸¤ä¸ªç‰ˆæœ¬çš„diagramæè¿°"""
    
    # Azure OpenAI é…ç½®
    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "https://linjl-ma65uv6u-eastus2.cognitiveservices.azure.com/")
    api_key = os.getenv("AZURE_OPENAI_API_KEY", "")
    deployment = "o3-DR"  # ç›´æ¥ä½¿ç”¨o3-DRéƒ¨ç½²
    api_version = "2025-01-01-preview"
    
    if not api_key:
        return {"error": "Azure OpenAI API key not found"}
    
    # æ„å»ºè¯·æ±‚URL
    url = f"{endpoint}openai/deployments/{deployment}/chat/completions?api-version={api_version}"
    headers = {"Content-Type": "application/json", "api-key": api_key}
    
    # ä½¿ç”¨promptæ¨¡æ¿
    prompt = prompt_manager.get_diagram_description_prompt(caption, context)

    payload = {
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "max_completion_tokens": 4000
    }
    
    try:
        data = make_api_request_with_retry(url, headers, payload, max_retries=3, delay=2.0, timeout=120)
        content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
        
        return {
            "summary": content,
            "success": True
        }
            
    except Exception as e:
        return {"error": f"API request failed: {str(e)}"}


def test_smart_markdown_paper(paper_dir: Path, paper_name: str):
    """æµ‹è¯•å•ä¸ªmarkdownè®ºæ–‡çš„æ™ºèƒ½diagramåˆ†æ"""
    print(f"\n{'='*60}")
    print(f"ğŸ“š æµ‹è¯•è®ºæ–‡: {paper_name}")
    print(f"{'='*60}")
    
    # æŸ¥æ‰¾markdownæ–‡ä»¶
    markdown_path = paper_dir / "vlm" / f"{paper_name}.md"
    if not markdown_path.exists():
        print(f"âŒ æœªæ‰¾åˆ°markdownæ–‡ä»¶: {markdown_path}")
        return None
    
    print(f"ğŸ“„ è¯»å–markdownæ–‡ä»¶: {markdown_path}")
    try:
        with open(markdown_path, 'r', encoding='utf-8') as f:
            markdown_content = f.read()
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return None
    
    # æå–æ‰€æœ‰å›¾ç‰‡
    print("\nğŸ” æå–æ‰€æœ‰å›¾ç‰‡...")
    all_figures = extract_all_figures_from_markdown(markdown_content)
    
    if not all_figures:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡")
        return {
            "paper_name": paper_name,
            "paper_dir": str(paper_dir),
            "total_figures": 0,
            "diagram_figures": 0,
            "diagram_figures_list": [],
            "results": []
        }
    
    print(f"âœ… æ‰¾åˆ° {len(all_figures)} ä¸ªå›¾ç‰‡")
    
    # ä½¿ç”¨GPTæ™ºèƒ½åˆ†ç±»å›¾ç‰‡
    print("\nğŸ¤– ä½¿ç”¨GPTæ™ºèƒ½åˆ†ç±»å›¾ç‰‡...")
    diagram_figures = classify_figures_with_gpt(all_figures, paper_name)
    
    if not diagram_figures:
        print("âŒ GPTæœªè¯†åˆ«å‡ºä»»ä½•diagramå›¾ç‰‡")
        return {
            "paper_name": paper_name,
            "paper_dir": str(paper_dir),
            "total_figures": len(all_figures),
            "diagram_figures": 0,
            "diagram_figures_list": [],
            "results": []
        }
    
    print(f"âœ… GPTè¯†åˆ«å‡º {len(diagram_figures)} ä¸ªdiagramå›¾ç‰‡")
    
    # åˆ†ææ¯ä¸ªdiagramå›¾ç‰‡
    results = []
    for i, figure in enumerate(diagram_figures, 1):
        print(f"\nğŸ“Š åˆ†ædiagramå›¾ç‰‡ {i}: {figure['id']}")
        print(f"   Caption: {figure['caption'][:100]}...")
        
        # æ„å»ºå›¾ç‰‡è·¯å¾„
        image_path = paper_dir / "vlm" / figure['src']
        
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not image_path.exists():
            print(f"   âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
            possible_paths = [
                paper_dir / "vlm" / "images" / figure['src'],
                paper_dir / "vlm" / figure['src'].replace('images/', ''),
                paper_dir / figure['src'],
                paper_dir / "images" / figure['src']
            ]
            
            found = False
            for possible_path in possible_paths:
                if possible_path.exists():
                    image_path = possible_path
                    found = True
                    print(f"   âœ… æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶: {image_path}")
                    break
            
            if not found:
                print(f"   âŒ æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡è·¯å¾„éƒ½ä¸å­˜åœ¨")
                continue
        
        # ä½¿ç”¨æ£€ç´¢æå–ç›¸å…³ä¸Šä¸‹æ–‡
        full_context = get_semantic_context_for_figure(markdown_content, figure['caption'])
        
        # è¾“å‡ºæå–åˆ°çš„ä¸Šä¸‹æ–‡ç”¨äºè°ƒè¯•
        print(f"   ğŸ“„ æå–åˆ°çš„ä¸Šä¸‹æ–‡:")
        print(f"   {'='*50}")
        print(f"   {full_context[:300]}...")
        print(f"   {'='*50}")
        
        # ä½¿ç”¨GPT-4oåˆ†æå›¾ç‰‡
        print("   ğŸ” ä½¿ç”¨GPT-4oåˆ†æå›¾ç‰‡...")
        diagram_analysis = analyze_diagram_with_gpt4o(str(image_path), figure['caption'], full_context)
        
        if "error" in diagram_analysis:
            print(f"   âŒ å›¾ç‰‡åˆ†æå¤±è´¥: {diagram_analysis['error']}")
            # å¦‚æœåªæ˜¯JSONè§£æå¤±è´¥ï¼Œä½†ä»ç„¶æœ‰åŸå§‹å“åº”ï¼Œå¯ä»¥ç»§ç»­å¤„ç†
            if diagram_analysis.get("raw_response"):
                print(f"   âš ï¸ ä½†æœ‰åŸå§‹å“åº”å¯ç”¨ï¼Œç»§ç»­å¤„ç†...")
            else:
                continue
        
        print("   âœ… å›¾ç‰‡åˆ†æå®Œæˆ")
        
        # ä½¿ç”¨GPT-o3ç”Ÿæˆæ€»ç»“
        print("   ğŸ” ä½¿ç”¨GPT-o3ç”Ÿæˆç»˜å›¾æŒ‡ä»¤...")
        summary_result = generate_diagram_description_with_o3(
            figure['caption'], 
            full_context
        )
        
        if "error" in summary_result:
            print(f"   âŒ æ€»ç»“ç”Ÿæˆå¤±è´¥: {summary_result['error']}")
            # å³ä½¿GPT-o3å¤±è´¥ï¼Œä¹Ÿä¿å­˜GPT-4oçš„åˆ†æç»“æœ
            summary_result = {"error": "GPT-o3 summary generation failed", "diagram_analysis_available": True}
        
        print("   âœ… ç»˜å›¾æŒ‡ä»¤ç”Ÿæˆå®Œæˆ")
        
        # è§£æGPT-o3çš„JSONå“åº”
        parsed_summary = {}
        if "summary" in summary_result:
            try:
                parsed_summary = json.loads(summary_result["summary"])
                print(f"   âœ… JSONè§£ææˆåŠŸï¼ŒåŒ…å«å­—æ®µ: {list(parsed_summary.keys())}")
            except json.JSONDecodeError as e:
                print(f"   âš ï¸ JSONè§£æå¤±è´¥: {e}")
                # å°è¯•ä»markdownä»£ç å—ä¸­æå–
                parsed_summary = extract_json_from_markdown(summary_result["summary"])
                print(f"   ğŸ“‹ æå–ç»“æœ: {parsed_summary}")
        
        # æ„å»ºåŒé˜¶æ®µè®­ç»ƒæ•°æ®
        # ç¬¬ä¸€é˜¶æ®µè¾“å‡º = ç¬¬äºŒé˜¶æ®µè¾“å…¥ (diagram description)
        analysis = diagram_analysis.get("diagram_analysis", {})
        nodes = analysis.get("nodes", [])
        relationships = analysis.get("relationships", [])
        
        # ä»nodesä¸­æå–main_componentsï¼ˆä½¿ç”¨æ–°çš„nodesç»“æ„ï¼‰
        main_components = []
        for node in nodes:
            if isinstance(node, dict):
                # ä¼˜å…ˆä½¿ç”¨contentï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨id
                content = node.get("content", node.get("id", ""))
                if content:
                    main_components.append(content)
            else:
                # å‘åå…¼å®¹ï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥æ·»åŠ 
                main_components.append(str(node))
        
        diagram_description = {
            "diagram_description": parsed_summary.get("diagram_description_long", ""),
            "diagram_description_long": parsed_summary.get("diagram_description_long", ""),
            "diagram_description_short": parsed_summary.get("diagram_description_short", ""),
            "diagram_type": analysis.get("diagram_type", ""),
            "main_components": main_components,
            "relationships": relationships
        }
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        data_quality = "valid"
        quality_issues = []
        
        # æ£€æŸ¥stage1æ•°æ®è´¨é‡
        if not diagram_description.get("diagram_description", "").strip():
            data_quality = "invalid"
            quality_issues.append("empty_diagram_description")
        
        # æ£€æŸ¥stage2æ•°æ®è´¨é‡
        if "error" in summary_result:
            data_quality = "invalid"
            quality_issues.append("stage2_generation_failed")
        
        # ä»GPT-o3ç»“æœä¸­è·å–é•¿çŸ­ä¸¤ä¸ªç‰ˆæœ¬çš„æè¿°
        diagram_desc_long = diagram_description.get("diagram_description_long", diagram_description.get("diagram_description", ""))
        diagram_desc_short = diagram_description.get("diagram_description_short", "")
        
        training_data = {
            # æ•°æ®è´¨é‡æ ‡ç­¾
            "data_quality": data_quality,
            "quality_issues": quality_issues,
            
            # ç¬¬ä¸€é˜¶æ®µè®­ç»ƒæ•°æ®: context + caption -> diagram description (GPT-o3)
            "stage1_input": {
                "context": full_context,
                "caption": figure['caption']
            },
            
            # ç¬¬äºŒé˜¶æ®µè®­ç»ƒæ•°æ®: diagram description -> thinking + image (GPT-4o)
            "stage2_input": {
                "diagram_description_long": diagram_desc_long,  # é•¿ç‰ˆæœ¬æè¿°
                "diagram_description_short": diagram_desc_short  # çŸ­ç‰ˆæœ¬æè¿°ï¼ˆé€šè¿‡GPTç”Ÿæˆï¼‰
            },
            "stage2_output": {
                "thinking_long": "",
                "thinking_short": "",
                "image_path": str(image_path)  # ç¬¬äºŒé˜¶æ®µè¾“å‡ºåŒ…å«å›¾ç‰‡è·¯å¾„
            }
        }
        
        # ä½¿ç”¨GPT-o3ç”Ÿæˆthinkingï¼Œç»¼åˆè§†è§‰åˆ†æå’Œpaper context
        print(f"   ğŸ” ä½¿ç”¨GPT-o3ç”Ÿæˆthinking...")
        thinking_result = generate_thinking_with_o3(
            figure['caption'], 
            full_context, 
            json.dumps(diagram_analysis, ensure_ascii=False)
        )
        
        
        judge_data_entry = None
        if "summary" in thinking_result:
            thinking_content = thinking_result["summary"]
            
            # è§£æGPT-o3çš„JSONå“åº”
            try:
                thinking_json = json.loads(thinking_content)
                thinking_long = thinking_json.get("thinking_long", "")
                thinking_short = thinking_json.get("thinking_short", "")
                print(f"   âœ… Thinkingç”ŸæˆæˆåŠŸ")
            except json.JSONDecodeError as e:
                print(f"   âš ï¸ Thinking JSONè§£æå¤±è´¥: {e}")
                # å°è¯•ä»markdownä»£ç å—ä¸­æå–
                thinking_json = extract_json_from_markdown(thinking_content)
                thinking_long = thinking_json.get("thinking_long", "")
                thinking_short = thinking_json.get("thinking_short", "")
            
            # æ£€æŸ¥thinkingæ˜¯å¦ä¸ºç©ºæˆ–åŒ…å«çœç•¥å·
            if not thinking_long.strip() or "..." in thinking_long:
                data_quality = "invalid"
                quality_issues.append("empty_or_incomplete_thinking_long")
            
            if not thinking_short.strip() or "..." in thinking_short:
                data_quality = "invalid"
                quality_issues.append("empty_or_incomplete_thinking_short")
            
            training_data["stage2_output"]["thinking_long"] = thinking_long
            training_data["stage2_output"]["thinking_short"] = thinking_short
            
            # æ›´æ–°æ•°æ®è´¨é‡æ ‡ç­¾
            training_data["data_quality"] = data_quality
            training_data["quality_issues"] = quality_issues
            
            # ä¸ºjudge_dataå‡†å¤‡è¯„åˆ†æ ‡å‡†(rubric)æ•°æ®
            analysis = diagram_analysis.get("diagram_analysis", {})
            nodes = analysis.get("nodes", [])
            groups = analysis.get("groups", [])
            relationships = analysis.get("relationships", [])
            visual_elements = analysis.get("visual_elements", {})
            standalone_nodes = analysis.get("standalone_nodes", [])
            
            # æå–æ‰€æœ‰èŠ‚ç‚¹ï¼ˆä½¿ç”¨æ–°çš„nodesç»“æ„ï¼‰
            all_nodes = []
            
            # ä»nodesæ•°ç»„ä¸­æå–èŠ‚ç‚¹ä¿¡æ¯
            for node in nodes:
                if isinstance(node, dict):
                    # ä½¿ç”¨æ–°çš„nodesç»“æ„ï¼šåŒ…å«id, type, content, attributes
                    node_info = {
                        "id": node.get("id", ""),
                        "type": node.get("type", ""),
                        "content": node.get("content", ""),
                        "attributes": node.get("attributes", {})
                    }
                    all_nodes.append(node_info)
                else:
                    # å‘åå…¼å®¹ï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥æ·»åŠ 
                    all_nodes.append(node)
            
            # æ·»åŠ ç‹¬ç«‹èŠ‚ç‚¹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            for standalone_node in standalone_nodes:
                if isinstance(standalone_node, str):
                    all_nodes.append(standalone_node)
                else:
                    all_nodes.append(standalone_node)
            
            # ä»groupsä¸­æå–èŠ‚ç‚¹å¼•ç”¨ï¼ˆä¿æŒå¼•ç”¨å…³ç³»ï¼‰
            group_node_refs = []
            for group in groups:
                group_node_refs.extend(group.get("nodes", []))
            
            judge_data_entry = {
                "image_info": {
                    "image_path": str(image_path),
                    "figure_id": figure['id'],
                    "figure_src": figure['src'],
                    "figure_caption": figure['caption']
                },
                "evaluation_rubric": {
                    "semantic_criteria": {
                        "critical_entities": all_nodes,
                        "critical_relationships": relationships,
                        "hierarchical_groups": groups,  # ç›´æ¥ä½¿ç”¨æ–°çš„groupsç»“æ„
                        "data_flow": extract_flow_from_relationships(relationships),
                        "dependencies": extract_dependencies_from_relationships(relationships)
                    },
                    "visual_criteria": {
                        "layout_requirements": visual_elements.get("layout", ""),
                        "color_scheme": visual_elements.get("colors", []),
                        "shape_requirements": visual_elements.get("shapes", [])
                    }
                },
                "reference_descriptions": {
                    "detailed_thinking": thinking_long,
                    "concise_thinking": thinking_short
                }
            }
            
            # ä¸å†æå–drawing instructionsï¼Œå› ä¸ºä¸éœ€è¦step-by-stepæè¿°
        
        result = {
            "training_data": training_data,
            "judge_data": judge_data_entry
        }
        results.append(result)
        
        # æ˜¾ç¤ºç»“æœ
        if "error" not in summary_result or summary_result.get("diagram_analysis_available"):
            print("   ğŸ“‹ ç”Ÿæˆçš„ç»˜å›¾æŒ‡ä»¤:")
            print("   " + "="*50)
            if "summary" in summary_result:
                full_response = summary_result.get('summary', '')
                print(f"   {full_response[:200]}...")
                print(f"   ğŸ“Š å®Œæ•´å“åº”é•¿åº¦: {len(full_response)} å­—ç¬¦")
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸¤ä¸ªå­—æ®µ
                if "diagram_description_short" in full_response:
                    print("   âœ… åŒ…å«çŸ­æè¿°å­—æ®µ")
                else:
                    print("   âŒ ç¼ºå°‘çŸ­æè¿°å­—æ®µ")
            else:
                print("   GPT-4oåˆ†æç»“æœå·²ä¿å­˜ï¼Œä½†GPT-o3æ€»ç»“ç”Ÿæˆå¤±è´¥")
            print("   " + "="*50)
    
    return {
        "paper_name": paper_name,
        "paper_dir": str(paper_dir),
        "total_figures": len(all_figures),
        "diagram_figures": len(diagram_figures),
        "diagram_figures_list": diagram_figures,  # åªä¿å­˜æœ‰ç”¨çš„diagramå›¾ç‰‡
        "results": results
    }


def main():
    """ä¸»å‡½æ•° - æµ‹è¯•æ‰€æœ‰markdownè®ºæ–‡çš„æ™ºèƒ½diagramåˆ†æ"""
    print("ğŸ¤– æ™ºèƒ½diagramåˆ†æ - ä½¿ç”¨GPTåˆ¤æ–­å›¾ç‰‡ç±»å‹")
    print("=" * 60)
    
    # è®ºæ–‡ç›®å½•
    papers_dir = Path(__file__).parent.parent.parent / "workspace" / "papers_markdown"
    
    # è·å–æ‰€æœ‰è®ºæ–‡ç›®å½•
    paper_dirs = [d for d in papers_dir.iterdir() if d.is_dir()]
    
    if not paper_dirs:
        print("âŒ æœªæ‰¾åˆ°è®ºæ–‡ç›®å½•")
        return
    
    print(f"ğŸ“š æ‰¾åˆ° {len(paper_dirs)} ä¸ªè®ºæ–‡ç›®å½•")
    
    # æµ‹è¯•æ¯ä¸ªè®ºæ–‡
    all_results = []
    for paper_dir in paper_dirs:
        paper_name = paper_dir.name
        result = test_smart_markdown_paper(paper_dir, paper_name)
        if result:
            all_results.append(result)
    
    # åˆ†ç¦»è®­ç»ƒæ•°æ®å’Œjudgeæ•°æ®
    training_data = []
    judge_data = []
    
    for result in all_results:
        for item in result["results"]:
            if item.get("training_data"):
                training_data.append(item["training_data"])
            if item.get("judge_data"):
                judge_data.append(item["judge_data"])
    
    # ä¿å­˜è®­ç»ƒæ•°æ®
    training_output_path = Path(__file__).parent / "diagram_training_data.json"
    with open(training_output_path, 'w', encoding='utf-8') as f:
        json.dump(training_data, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜judgeæ•°æ®
    judge_output_path = Path(__file__).parent / "diagram_judge_data.json"
    with open(judge_output_path, 'w', encoding='utf-8') as f:
        json.dump(judge_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {training_output_path}")
    print(f"ğŸ’¾ Judgeæ•°æ®å·²ä¿å­˜åˆ°: {judge_output_path}")
    
    # ç»Ÿè®¡ç»“æœ
    print(f"\n{'='*60}")
    print("ğŸ“Š æ™ºèƒ½åˆ†æç»“æœç»Ÿè®¡")
    print(f"{'='*60}")
    
    total_papers = len(all_results)
    total_figures = sum(r["total_figures"] for r in all_results)
    total_diagrams = sum(r["diagram_figures"] for r in all_results)
    successful_analyses = 0
    valid_training_data = 0
    invalid_training_data = 0
    
    for result in all_results:
        paper_name = result["paper_name"]
        total_figs = result["total_figures"]
        diagram_figs = result["diagram_figures"]
        successful = sum(1 for r in result["results"] if "error" not in r.get("drawing_summary", {}))
        
        # ç»Ÿè®¡æ•°æ®è´¨é‡
        valid_count = sum(1 for r in result["results"] if r.get("training_data", {}).get("data_quality") == "valid")
        invalid_count = sum(1 for r in result["results"] if r.get("training_data", {}).get("data_quality") == "invalid")
        
        print(f"ğŸ“š {paper_name}:")
        print(f"   ğŸ“Š æ€»å›¾ç‰‡æ•°: {total_figs}")
        print(f"   ğŸ¤– GPTè¯†åˆ«diagram: {diagram_figs}")
        print(f"   âœ… æˆåŠŸåˆ†æ: {successful}/{diagram_figs}")
        print(f"   âœ… æœ‰æ•ˆè®­ç»ƒæ•°æ®: {valid_count}")
        print(f"   âŒ æ— æ•ˆè®­ç»ƒæ•°æ®: {invalid_count}")
        print()
        
        successful_analyses += successful
        valid_training_data += valid_count
        invalid_training_data += invalid_count
    
    print(f"ğŸ¯ æ€»ä½“ç»Ÿè®¡:")
    print(f"   ğŸ“š æµ‹è¯•è®ºæ–‡æ•°: {total_papers}")
    print(f"   ğŸ“Š æ€»å›¾ç‰‡æ•°: {total_figures}")
    print(f"   ğŸ¤– GPTè¯†åˆ«diagram: {total_diagrams}")
    print(f"   âœ… æˆåŠŸåˆ†æ: {successful_analyses}/{total_diagrams}")
    print(f"   ğŸ“ˆ æˆåŠŸç‡: {successful_analyses/total_diagrams*100:.1f}%" if total_diagrams > 0 else "   ğŸ“ˆ æˆåŠŸç‡: N/A")
    print(f"   ğŸ¯ è¯†åˆ«ç‡: {total_diagrams/total_figures*100:.1f}%" if total_figures > 0 else "   ğŸ¯ è¯†åˆ«ç‡: N/A")
    print(f"   âœ… æœ‰æ•ˆè®­ç»ƒæ•°æ®: {valid_training_data}")
    print(f"   âŒ æ— æ•ˆè®­ç»ƒæ•°æ®: {invalid_training_data}")
    print(f"   ğŸ“Š æ•°æ®è´¨é‡ç‡: {valid_training_data/(valid_training_data+invalid_training_data)*100:.1f}%" if (valid_training_data+invalid_training_data) > 0 else "   ğŸ“Š æ•°æ®è´¨é‡ç‡: N/A")
    
    print(f"\nğŸ’¾ å®Œæ•´è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {training_output_path}")


if __name__ == "__main__":
    main()
