#!/usr/bin/env python3
"""
Hybrid Diagram Figure Reasoner - æ”¯æŒå¤šAPIæºçš„æ™ºèƒ½å›¾è¡¨åˆ†æå™¨

åŠŸèƒ½ï¼š
1. æ”¯æŒåˆ‡æ¢ä¸åŒçš„GPT-4oæ¥å£æºï¼ˆPapyrus å’Œ Azure OpenAIï¼‰
2. æå–æ‰€æœ‰å›¾ç‰‡çš„caption
3. ä½¿ç”¨GPTåˆ¤æ–­å“ªäº›æ˜¯diagram
4. åªåˆ†æè¢«GPTè¯†åˆ«ä¸ºdiagramçš„å›¾ç‰‡
5. ç”Ÿæˆç»˜å›¾æŒ‡ä»¤

æ”¯æŒçš„APIæºï¼š
- Papyrus (Microsoftå†…éƒ¨APIï¼Œé»˜è®¤)
- Azure OpenAI (å¤‡é€‰)
"""

import os
import re
import json
import requests
import base64
import time
from pathlib import Path
from typing import Dict, List, Optional
from enum import Enum

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, AzureOpenAIEmbeddings
from prompts_template.prompt_manager import prompt_manager
from langchain.schema import Document
HAS_LANGCHAIN = True

# SentenceTransformer imports for local embeddings (å¼ºåˆ¶ä½¿ç”¨ï¼Œé¿å…Azure APIé™åˆ¶)
try:
    from sentence_transformers import SentenceTransformer
    import torch
    HAS_SENTENCE_TRANSFORMERS = True
    print("âœ… SentenceTransformer å·²å°±ç»ª - å°†å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹é¿å…APIé™åˆ¶")
except ImportError:
    HAS_SENTENCE_TRANSFORMERS = False
    print("âŒ SentenceTransformer æœªå®‰è£…ï¼")
    print("ğŸš« æˆ‘ä»¬å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹ä»¥é¿å…Azure API rateé™åˆ¶")
    print("ğŸ“¦ è¯·å®‰è£…: pip install sentence-transformers torch")

from dotenv import load_dotenv
HAS_DOTENV = True

# Azure Identity imports for Papyrus
try:
    from azure.identity import DefaultAzureCredential, AzureCliCredential, ManagedIdentityCredential
    HAS_AZURE_IDENTITY = True
except ImportError:
    HAS_AZURE_IDENTITY = False
    print("âš ï¸ Azure Identity not available. Papyrus API will not work.")


class APISource(Enum):
    """APIæºæšä¸¾"""
    AZURE_OPENAI = "azure_openai"
    PAPYRUS = "papyrus"


class HybridDiagramReasoner:
    """æ··åˆå›¾è¡¨åˆ†æå™¨ï¼Œæ”¯æŒå¤šAPIæº"""
    
    def __init__(self, api_source: APISource = APISource.PAPYRUS):
        self.api_source = api_source
        self.last_api_call_time = 0
        self.api_call_interval = 1.0  # 1ç§’é—´éš”ï¼Œé¿å…rate limit
        self.load_env_vars()
        self.setup_api_config()
    
    def load_env_vars(self):
        """åŠ è½½ç¯å¢ƒå˜é‡"""
        # å°è¯•ä»å¤šä¸ªä½ç½®åŠ è½½ .env æ–‡ä»¶
        env_paths = [
            Path(__file__).parent / ".env",
            Path(__file__).parent.parent / ".env",
            Path(__file__).parent.parent.parent / "CoTT" / ".env",
            Path(__file__).parent.parent.parent / "CoTT" / ".env_old",
            Path("/Users/suny0a/Proj/MM-Reasoning/CoTT/.env"),  # ç»å¯¹è·¯å¾„
            Path("/home/t2vg-a100-G2-0/yasheng/CoTT/.env"),  # æœåŠ¡å™¨è·¯å¾„
        ]
        
        if HAS_DOTENV:
            for env_path in env_paths:
                if env_path.exists():
                    print(f"ğŸ“„ åŠ è½½ç¯å¢ƒå˜é‡: {env_path}")
                    load_dotenv(env_path)
                    # éªŒè¯å…³é”®ç¯å¢ƒå˜é‡æ˜¯å¦åŠ è½½æˆåŠŸ
                    api_key = os.getenv("AZURE_OPENAI_API_KEY")
                    if api_key:
                        print(f"âœ… Azure OpenAI API Key å·²åŠ è½½: {api_key[:20]}...")
                    else:
                        print("âŒ Azure OpenAI API Key æœªæ‰¾åˆ°")
                    return True
        else:
            # ç®€å•çš„ç¯å¢ƒå˜é‡åŠ è½½
            for env_path in env_paths:
                if env_path.exists():
                    print(f"ğŸ“„ ä»æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡: {env_path}")
                    with open(env_path, 'r') as f:
                        for line in f:
                            line = line.strip()
                            if line and not line.startswith('#') and '=' in line:
                                key, value = line.split('=', 1)
                                os.environ[key] = value
                    
                    # éªŒè¯å…³é”®ç¯å¢ƒå˜é‡æ˜¯å¦åŠ è½½æˆåŠŸ
                    api_key = os.getenv("AZURE_OPENAI_API_KEY")
                    if api_key:
                        print(f"âœ… Azure OpenAI API Key å·²åŠ è½½: {api_key[:20]}...")
                    else:
                        print("âŒ Azure OpenAI API Key æœªæ‰¾åˆ°")
                    return True
        
        print("âš ï¸ æœªæ‰¾åˆ°ç¯å¢ƒå˜é‡æ–‡ä»¶")
        return False
    
    def setup_api_config(self):
        """è®¾ç½®APIé…ç½®"""
        if self.api_source == APISource.AZURE_OPENAI:
            self.endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "https://linjl-ma65uv6u-eastus2.cognitiveservices.azure.com/")
            self.api_key = os.getenv("AZURE_OPENAI_API_KEY", "")
            self.deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4o")
            self.api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
            
            if not self.api_key:
                print("âŒ Azure OpenAI API key not found")
                return False
            
            print(f"ğŸ”— ä½¿ç”¨ Azure OpenAI API")
            print(f"   Endpoint: {self.endpoint}")
            print(f"   Deployment: {self.deployment}")
            
        elif self.api_source == APISource.PAPYRUS:
            if not HAS_AZURE_IDENTITY:
                print("âŒ Azure Identity not available for Papyrus API")
                return False
            
            # ä»ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼è·å–Papyrusé…ç½®
            self.papyrus_endpoint = os.getenv("PAPYRUS_ENDPOINT", "https://WestUS2Large.papyrus.binginternal.com/chat/completions")
            self.verify_scope = os.getenv("PAPYRUS_VERIFY_SCOPE", "api://5fe538a8-15d5-4a84-961e-be66cd036687/.default")
            self.client_id = os.getenv("PAPYRUS_CLIENT_ID", "d5702df1-96d9-4195-83a3-e44d8b0a0601")
            
            # å°è¯•ä¸åŒçš„è®¤è¯æ–¹å¼
            self.access_token = None
            self.setup_papyrus_auth()
            
            if not self.access_token:
                print("âŒ Failed to get Papyrus access token")
                return False
            
            print(f"ğŸ”— ä½¿ç”¨ Papyrus API")
            print(f"   Endpoint: {self.papyrus_endpoint}")
            print(f"   Access token: {self.access_token[:20]}...")
        
        return True
    
    def setup_papyrus_auth(self):
        """è®¾ç½®Papyrusè®¤è¯"""
        # ä¼˜å…ˆå°è¯•ManagedIdentityCredentialï¼ˆä¸papyrus_on_vm_2.pyä¿æŒä¸€è‡´ï¼‰
        try:
            print("ğŸ” å°è¯•ä½¿ç”¨ Managed Identity è®¤è¯...")
            cred = ManagedIdentityCredential(client_id=self.client_id)
            self.access_token = cred.get_token(self.verify_scope).token
            print("âœ… Managed Identity è®¤è¯æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ Managed Identity è®¤è¯å¤±è´¥: {e}")
        
        try:
            # å°è¯•ä½¿ç”¨DefaultAzureCredential
            print("ğŸ” å°è¯•ä½¿ç”¨ Default Azure è®¤è¯...")
            cred = DefaultAzureCredential()
            self.access_token = cred.get_token(self.verify_scope).token
            print("âœ… Default Azure è®¤è¯æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ Default Azure è®¤è¯å¤±è´¥: {e}")
        
        try:
            # æœ€åå°è¯•ä½¿ç”¨AzureCliCredential
            print("ğŸ” å°è¯•ä½¿ç”¨ Azure CLI è®¤è¯...")
            cred = AzureCliCredential()
            self.access_token = cred.get_token(self.verify_scope).token
            print("âœ… Azure CLI è®¤è¯æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ Azure CLI è®¤è¯å¤±è´¥: {e}")
        
        return False
    
    def get_api_headers(self) -> Dict[str, str]:
        """è·å–APIè¯·æ±‚å¤´"""
        if self.api_source == APISource.AZURE_OPENAI:
            return {
                "Content-Type": "application/json",
                "api-key": self.api_key
            }
        elif self.api_source == APISource.PAPYRUS:
            return {
                "Authorization": f"Bearer {self.access_token}",
                "Content-Type": "application/json",
                "papyrus-model-name": os.getenv("PAPYRUS_MODEL_NAME", "gpt4ovision-batch"),
                "papyrus-timeout-ms": os.getenv("PAPYRUS_TIMEOUT_MS", "30000"),
                "papyrus-quota-id": os.getenv("PAPYRUS_QUOTA_ID", "msftaicopilot/windowsdata"),
            }
        return {}
    
    def get_api_url(self) -> str:
        """è·å–APIè¯·æ±‚URL"""
        if self.api_source == APISource.AZURE_OPENAI:
            return f"{self.endpoint}openai/deployments/{self.deployment}/chat/completions?api-version={self.api_version}"
        elif self.api_source == APISource.PAPYRUS:
            return self.papyrus_endpoint
        return ""
    
    def make_api_request_with_retry(self, payload: Dict, max_retries: int = 3, delay: float = 2.0) -> Dict:
        """å¸¦é‡è¯•æœºåˆ¶çš„APIè¯·æ±‚"""
        for attempt in range(max_retries):
            try:
                # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œé¿å…rate limit
                current_time = time.time()
                time_since_last_call = current_time - self.last_api_call_time
                if time_since_last_call < self.api_call_interval:
                    sleep_time = self.api_call_interval - time_since_last_call
                    print(f"   â³ ç­‰å¾… {sleep_time:.1f} ç§’é¿å…rate limit...")
                    time.sleep(sleep_time)
                
                url = self.get_api_url()
                headers = self.get_api_headers()
                
                # å¦‚æœæ˜¯Papyrus APIä¸”tokenå¯èƒ½è¿‡æœŸï¼Œé‡æ–°è·å–token
                if self.api_source == APISource.PAPYRUS and attempt > 0:
                    print(f"   ğŸ”„ é‡è¯• {attempt + 1}/{max_retries}ï¼Œé‡æ–°è·å–token...")
                    self.setup_papyrus_auth()
                    headers = self.get_api_headers()
                
                response = requests.post(url, headers=headers, json=payload, timeout=180)
                self.last_api_call_time = time.time()  # æ›´æ–°æœ€åè°ƒç”¨æ—¶é—´
                
                if response.status_code == 401:
                    print(f"   âš ï¸ 401é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries})")
                    if attempt < max_retries - 1:
                        print(f"   â³ ç­‰å¾… {delay} ç§’åé‡è¯•...")
                        time.sleep(delay)
                        delay *= 1.5  # æŒ‡æ•°é€€é¿
                        continue
                    else:
                        response.raise_for_status()
                elif response.status_code == 429:
                    print(f"   âš ï¸ 429 Rate Limit (å°è¯• {attempt + 1}/{max_retries})")
                    if attempt < max_retries - 1:
                        wait_time = delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                        print(f"   â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    else:
                        response.raise_for_status()
                else:
                    response.raise_for_status()
                
                return response.json()
                
            except requests.exceptions.RequestException as e:
                print(f"   âŒ APIè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    print(f"   â³ ç­‰å¾… {delay} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    delay *= 1.5
                else:
                    raise e
        
        return {"error": f"API request failed after {max_retries} attempts"}
    
    def extract_all_figures_from_markdown(self, markdown_content: str) -> List[Dict]:
        """æå–æ‰€æœ‰å›¾ç‰‡ä¿¡æ¯ï¼Œä¸åšä»»ä½•è¿‡æ»¤"""
        figures = []
        seen_images = set()  # é¿å…é‡å¤æå–
        
        # æ‰¾åˆ°é™„å½•å¼€å§‹çš„ä½ç½®ï¼ˆæ’é™¤é™„å½•å†…å®¹ï¼‰
        lines = markdown_content.split('\n')
        appendix_start_idx = len(lines)  # é»˜è®¤æ²¡æœ‰é™„å½•
        for i, line in enumerate(lines):
            line_lower = line.lower().strip()
            # æ£€æŸ¥æ˜¯å¦æ˜¯é™„å½•æ ‡é¢˜
            if (line_lower.startswith('# appendix') or 
                line_lower.startswith('## appendix') or
                line_lower.startswith('### appendix') or
                line_lower.startswith('# supplementary') or
                line_lower.startswith('## supplementary') or
                line_lower.startswith('### supplementary') or
                'appendix' in line_lower and line_lower.startswith('#')):
                appendix_start_idx = i
                break
        
        # åªå¤„ç†æ­£æ–‡éƒ¨åˆ†ï¼ˆæ’é™¤é™„å½•ï¼‰
        main_content = '\n'.join(lines[:appendix_start_idx])
        
        # æŸ¥æ‰¾markdownæ ¼å¼çš„å›¾ç‰‡å¼•ç”¨: ![](path) æˆ– ![alt](path)
        image_pattern = r'!\[([^\]]*)\]\(([^)]+\.(?:jpg|jpeg|png|gif|bmp|svg))\)'
        image_matches = re.findall(image_pattern, main_content, re.IGNORECASE)
        
        # å¤„ç†æ¯ä¸ªå›¾ç‰‡å¼•ç”¨
        for alt_text, image_path in image_matches:
            # é¿å…é‡å¤å¤„ç†
            if image_path in seen_images:
                continue
            seen_images.add(image_path)
            
            # æŸ¥æ‰¾è¿™ä¸ªå›¾ç‰‡åé¢çš„caption
            caption = ""
            image_pos = main_content.find(f"![{alt_text}]({image_path})")
            if image_pos != -1:
                # æŸ¥æ‰¾å›¾ç‰‡åçš„æ–‡æœ¬ä½œä¸ºcaption
                after_image = main_content[image_pos + len(f"![{alt_text}]({image_path})"):]
                # æŸ¥æ‰¾ä¸‹ä¸€è¡Œæˆ–æ®µè½ä½œä¸ºcaption
                lines_after = after_image.split('\n')
                for line in lines_after:
                    line = line.strip()
                    if line and not line.startswith('![') and not line.startswith('#'):
                        # æ£€æŸ¥æ˜¯å¦æ˜¯Figureå¼€å¤´çš„caption
                        if line.lower().startswith('figure'):
                            caption = line
                            break
                        # æˆ–è€…å–ç¬¬ä¸€è¡Œéç©ºæ–‡æœ¬ä½œä¸ºcaption
                        elif not caption:
                            caption = line
                        # å¦‚æœé‡åˆ°ä¸‹ä¸€ä¸ªå›¾ç‰‡æˆ–æ ‡é¢˜ï¼Œåœæ­¢
                        if line.startswith('![') or line.startswith('#'):
                            break
            
            figures.append({
                'id': f"figure_{len(figures) + 1}",
                'src': image_path,
                'caption': caption or alt_text or f"Figure {len(figures) + 1}",
                'alt_text': alt_text
            })
        
        return figures
    
    def classify_figures_with_gpt(self, figures: List[Dict], paper_title: str = "") -> List[Dict]:
        """ä½¿ç”¨GPTåˆ¤æ–­å“ªäº›å›¾ç‰‡æ˜¯diagram"""
        
        if not self.setup_api_config():
            print("âŒ APIé…ç½®å¤±è´¥")
            return []
        
        # æ„å»ºæ‰€æœ‰å›¾ç‰‡çš„captionä¿¡æ¯
        figure_info = []
        for i, fig in enumerate(figures, 1):
            figure_info.append(f"Figure {i}: {fig['caption']}")
        
        figures_text = "\n".join(figure_info)
        
        # ä½¿ç”¨promptæ¨¡æ¿
        prompt = prompt_manager.get_figure_classification_prompt(paper_title, figures_text)

        payload = {
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "max_tokens": 1000,
            "temperature": 0.1
        }
        
        try:
            data = self.make_api_request_with_retry(payload, max_retries=3, delay=2.0)
            content = data.get("choices", [{}])[0].get("message", {}).get("content", "{}")
            
            # å°è¯•è§£æJSON
            try:
                # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
                result = json.loads(content)
            except json.JSONDecodeError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æå–markdownä»£ç å—ä¸­çš„JSON
                try:
                    # æŸ¥æ‰¾```jsonå’Œ```ä¹‹é—´çš„å†…å®¹ï¼ˆæ›´å®½æ¾çš„åŒ¹é…ï¼‰
                    json_match = re.search(r'```json\s*\n?(.*?)\n?```', content, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1).strip()
                        result = json.loads(json_content)
                    else:
                        # å°è¯•æŸ¥æ‰¾```å’Œ```ä¹‹é—´çš„å†…å®¹ï¼ˆæ²¡æœ‰jsonæ ‡è®°ï¼‰
                        code_match = re.search(r'```\s*\n?(.*?)\n?```', content, re.DOTALL)
                        if code_match:
                            json_content = code_match.group(1).strip()
                            result = json.loads(json_content)
                        else:
                            print(f"   âŒ æ— æ³•æ‰¾åˆ°JSONå†…å®¹: {content}")
                            return []
                except json.JSONDecodeError as e:
                    print(f"   âŒ æ— æ³•è§£æGPTåˆ†ç±»ç»“æœ: {content}")
                    print(f"   ğŸ” JSONè§£æé”™è¯¯: {e}")
                    return []
            
            # è§£ææˆåŠŸï¼Œæå–ç»“æœ
            diagram_figure_numbers = result.get("diagram_figures", [])
            reasoning = result.get("reasoning", "")
            
            print(f"   ğŸ¤– GPTåˆ†ç±»ç»“æœ: {reasoning}")
            
            # æ ¹æ®GPTçš„åˆ†ç±»ç»“æœç­›é€‰å›¾ç‰‡
            diagram_figures = []
            for i, fig in enumerate(figures, 1):
                if i in diagram_figure_numbers:
                    fig['type'] = 'diagram'
                    fig['gpt_reasoning'] = reasoning
                    diagram_figures.append(fig)
            
            return diagram_figures
                
        except Exception as e:
            print(f"   âŒ GPTåˆ†ç±»å¤±è´¥: {str(e)}")
            return []
    
    def encode_image(self, image_path: str) -> str:
        """å°†å›¾ç‰‡ç¼–ç ä¸ºbase64"""
        try:
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')
        except Exception as e:
            print(f"   âŒ å›¾ç‰‡ç¼–ç å¤±è´¥: {e}")
            return ""
    
    def analyze_diagram_with_gpt4o(self, image_path: str, caption: str, context: str) -> Dict:
        """ä½¿ç”¨GPT-4oåˆ†ædiagramå›¾ç‰‡å†…å®¹"""
        
        if not self.setup_api_config():
            return {"error": "APIé…ç½®å¤±è´¥"}
        
        # ç¼–ç å›¾ç‰‡
        base64_image = self.encode_image(image_path)
        if not base64_image:
            return {"error": "Failed to encode image"}
        
        # ä½¿ç”¨promptæ¨¡æ¿
        prompt = prompt_manager.get_diagram_analysis_prompt(caption, context)

        payload = {
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": prompt
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            "max_tokens": 3000,
            "temperature": 0.1
        }
        
        try:
            data = self.make_api_request_with_retry(payload, max_retries=3, delay=3.0)
            content = data.get("choices", [{}])[0].get("message", {}).get("content", "{}")
            
            # å°è¯•è§£æJSON
            try:
                # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
                result = json.loads(content)
                return result
            except json.JSONDecodeError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æå–markdownä»£ç å—ä¸­çš„JSON
                try:
                    # æŸ¥æ‰¾```jsonå’Œ```ä¹‹é—´çš„å†…å®¹ï¼ˆæ›´å®½æ¾çš„åŒ¹é…ï¼‰
                    json_match = re.search(r'```json\s*\n?(.*?)\n?```', content, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1).strip()
                        result = json.loads(json_content)
                        return result
                    else:
                        # å°è¯•æŸ¥æ‰¾```å’Œ```ä¹‹é—´çš„å†…å®¹ï¼ˆæ²¡æœ‰jsonæ ‡è®°ï¼‰
                        code_match = re.search(r'```\s*\n?(.*?)\n?```', content, re.DOTALL)
                        if code_match:
                            json_content = code_match.group(1).strip()
                            result = json.loads(json_content)
                            return result
                        else:
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»£ç å—ï¼Œå°è¯•æ¸…ç†å†…å®¹åè§£æ
                            cleaned_content = content.strip()
                            # ç§»é™¤å¯èƒ½çš„markdownæ ¼å¼
                            cleaned_content = re.sub(r'^```.*?\n', '', cleaned_content, flags=re.DOTALL)
                            cleaned_content = re.sub(r'\n```.*?$', '', cleaned_content, flags=re.DOTALL)
                            result = json.loads(cleaned_content)
                            return result
                except json.JSONDecodeError as e:
                    # å¦‚æœè¿˜æ˜¯è§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹ç”¨äºè°ƒè¯•
                    print(f"   âš ï¸ JSONè§£æå¤±è´¥ï¼ŒåŸå§‹å“åº”: {content[:200]}...")
                    print(f"   ğŸ” JSONè§£æé”™è¯¯: {e}")
                    return {
                        "raw_response": content,
                        "error": "Failed to parse JSON response",
                        "diagram_analysis_available": False
                    }
                
        except Exception as e:
            return {"error": f"API request failed: {str(e)}"}
    
    def switch_api_source(self, new_source: APISource):
        """åˆ‡æ¢APIæº"""
        print(f"ğŸ”„ åˆ‡æ¢APIæº: {self.api_source.value} -> {new_source.value}")
        self.api_source = new_source
        return self.setup_api_config()
    
    def test_api_connection(self) -> bool:
        """æµ‹è¯•APIè¿æ¥"""
        print(f"ğŸ” æµ‹è¯• {self.api_source.value} APIè¿æ¥...")
        
        test_payload = {
            "messages": [
                {
                    "role": "user",
                    "content": "Hello, this is a test message."
                }
            ],
            "max_tokens": 10,
            "temperature": 0.1
        }
        
        try:
            url = self.get_api_url()
            headers = self.get_api_headers()
            
            response = requests.post(url, headers=headers, json=test_payload, timeout=30)
            response.raise_for_status()
            
            print(f"âœ… {self.api_source.value} APIè¿æ¥æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ {self.api_source.value} APIè¿æ¥å¤±è´¥: {str(e)}")
            return False


def extract_json_from_markdown(content: str) -> Dict:
    """ä»markdownå†…å®¹ä¸­æå–JSON"""
    try:
        # æŸ¥æ‰¾```jsonå’Œ```ä¹‹é—´çš„å†…å®¹
        json_match = re.search(r'```json\s*\n(.*?)\n```', content, re.DOTALL)
        if json_match:
            json_content = json_match.group(1)
            return json.loads(json_content)
        
        # å°è¯•æŸ¥æ‰¾```å’Œ```ä¹‹é—´çš„å†…å®¹ï¼ˆæ²¡æœ‰jsonæ ‡è®°ï¼‰
        code_match = re.search(r'```\s*\n(.*?)\n```', content, re.DOTALL)
        if code_match:
            json_content = code_match.group(1)
            return json.loads(json_content)
        
        # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥è§£æ
        return json.loads(content)
        
    except json.JSONDecodeError:
        return {}


def generate_diagram_description_with_o3(caption: str, context: str) -> Dict:
    """ä½¿ç”¨GPT-o3åŒæ—¶ç”Ÿæˆé•¿çŸ­ä¸¤ä¸ªç‰ˆæœ¬çš„diagramæè¿°"""
    
    # Azure OpenAI é…ç½®
    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "https://linjl-ma65uv6u-eastus2.cognitiveservices.azure.com/")
    api_key = os.getenv("AZURE_OPENAI_API_KEY", "")
    deployment = "o3-DR"  # ç›´æ¥ä½¿ç”¨o3-DRéƒ¨ç½²
    api_version = "2025-01-01-preview"
    
    if not api_key:
        return {"error": "Azure OpenAI API key not found"}
    
    # æ„å»ºè¯·æ±‚URL
    url = f"{endpoint}openai/deployments/{deployment}/chat/completions?api-version={api_version}"
    headers = {"Content-Type": "application/json", "api-key": api_key}
    
    # ä½¿ç”¨promptæ¨¡æ¿
    prompt = prompt_manager.get_diagram_description_prompt(caption, context)

    payload = {
        "model": "GPT4oVision",
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "max_completion_tokens": 4000,
        "stream": False
    }
    
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=120)
        response.raise_for_status()
        
        data = response.json()
        content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
        
        return {
            "summary": content,
            "success": True
        }
            
    except Exception as e:
        return {"error": f"API request failed: {str(e)}"}


def generate_thinking_with_o3(caption: str, context: str, visual_analysis: str) -> Dict:
    """ä½¿ç”¨GPT-o3ç”Ÿæˆthinkingæè¿°ï¼Œç»¼åˆè§†è§‰åˆ†æå’Œpaper context"""
    
    # Azure OpenAI é…ç½®
    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "https://linjl-ma65uv6u-eastus2.cognitiveservices.azure.com/")
    api_key = os.getenv("AZURE_OPENAI_API_KEY", "")
    deployment = "o3-DR"  # ä½¿ç”¨o3-DRéƒ¨ç½²
    api_version = "2025-01-01-preview"
    
    if not api_key:
        return {"error": "Azure OpenAI API key not found"}
    
    # æ„å»ºURLå’Œheaders
    url = f"{endpoint}/openai/deployments/{deployment}/chat/completions?api-version={api_version}"
    headers = {
        "Content-Type": "application/json",
        "api-key": api_key
    }
    
    # ä½¿ç”¨prompt_managerè·å–prompt
    prompt = prompt_manager.get_thinking_generation_prompt(caption, context, visual_analysis)
    
    payload = {
        "model": "GPT4oVision",
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "max_completion_tokens": 4000,
        "stream": False
    }
    
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=120)
        response.raise_for_status()
        
        data = response.json()
        content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
        
        return {
            "summary": content,
            "success": True
        }
            
    except Exception as e:
        return {"error": f"API request failed: {str(e)}"}




def show_setup_help():
    """æ˜¾ç¤ºè®¾ç½®å¸®åŠ©ä¿¡æ¯"""
    print("\nğŸ”§ è®¾ç½®å¸®åŠ©")
    print("=" * 50)
    print("ğŸš« é‡è¦æ›´æ–°: æˆ‘ä»¬å¼ºåˆ¶ä½¿ç”¨SentenceTransformeræœ¬åœ°æ¨¡å‹é¿å…Azure APIé™åˆ¶")
    print()
    print("ğŸ“¦ å¿…éœ€ä¾èµ– (å¼ºåˆ¶å®‰è£…):")
    print("   pip install sentence-transformers torch")
    print("   æˆ–è¿è¡Œ: python install_sentence_transformer.py")
    print()
    print("ğŸ”— APIé…ç½® (ç”¨äºGPTåˆ†æï¼Œä¸ç”¨äºembedding):")
    print("1. Papyrus API (é»˜è®¤ï¼Œæ¨è):")
    print("   export PAPYRUS_ENDPOINT='https://WestUS2Large.papyrus.binginternal.com/chat/completions'")
    print("   export PAPYRUS_VERIFY_SCOPE='api://5fe538a8-15d5-4a84-961e-be66cd036687/.default'")
    print("   export PAPYRUS_CLIENT_ID='d5702df1-96d9-4195-83a3-e44d8b0a0601'")
    print()
    print("2. å¤‡é€‰ Azure OpenAI API:")
    print("   export AZURE_OPENAI_ENDPOINT='https://your-endpoint.cognitiveservices.azure.com/'")
    print("   export AZURE_OPENAI_API_KEY='your-api-key-here'")
    print("   export AZURE_OPENAI_DEPLOYMENT='gpt-4o'")
    print()
    print("3. å¯¹äºPapyrus APIï¼Œéœ€è¦å®‰è£…:")
    print("   pip install azure-identity")
    print()
    print("4. å¦‚æœAzure CLI tokenè¿‡æœŸï¼Œé‡æ–°ç™»å½•:")
    print("   az login")
    print()
    print("ğŸ’¡ ä¼˜åŠ¿:")
    print("   âœ… æ— API rateé™åˆ¶")
    print("   âœ… æœ¬åœ°å¤„ç†ï¼Œéšç§å®‰å…¨")
    print("   âœ… æ¨¡å‹ç¼“å­˜ï¼Œæ€§èƒ½ä¼˜åŒ–")
    print("   âœ… ç¦»çº¿å¯ç”¨")
    print()


def test_smart_markdown_paper(paper_dir: Path, paper_name: str, reasoner: HybridDiagramReasoner):
    """æµ‹è¯•å•ä¸ªmarkdownè®ºæ–‡çš„æ™ºèƒ½diagramåˆ†æ"""
    print(f"\n{'='*60}")
    print(f"ğŸ“š æµ‹è¯•è®ºæ–‡: {paper_name}")
    print(f"{'='*60}")
    
    # æŸ¥æ‰¾markdownæ–‡ä»¶
    markdown_path = paper_dir / "vlm" / f"{paper_name}.md"
    if not markdown_path.exists():
        print(f"âŒ æœªæ‰¾åˆ°markdownæ–‡ä»¶: {markdown_path}")
        return None
    
    print(f"ğŸ“„ è¯»å–markdownæ–‡ä»¶: {markdown_path}")
    try:
        with open(markdown_path, 'r', encoding='utf-8') as f:
            markdown_content = f.read()
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return None
    
    # æå–æ‰€æœ‰å›¾ç‰‡
    print("\nğŸ” æå–æ‰€æœ‰å›¾ç‰‡...")
    all_figures = reasoner.extract_all_figures_from_markdown(markdown_content)
    
    if not all_figures:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡")
        return {
            "paper_name": paper_name,
            "paper_dir": str(paper_dir),
            "total_figures": 0,
            "diagram_figures": 0,
            "diagram_figures_list": [],
            "results": []
        }
    
    print(f"âœ… æ‰¾åˆ° {len(all_figures)} ä¸ªå›¾ç‰‡")
    
    # ä½¿ç”¨GPTæ™ºèƒ½åˆ†ç±»å›¾ç‰‡
    print("\nğŸ¤– ä½¿ç”¨GPTæ™ºèƒ½åˆ†ç±»å›¾ç‰‡...")
    diagram_figures = reasoner.classify_figures_with_gpt(all_figures, paper_name)
    
    if not diagram_figures:
        print("âŒ GPTæœªè¯†åˆ«å‡ºä»»ä½•diagramå›¾ç‰‡")
        return {
            "paper_name": paper_name,
            "paper_dir": str(paper_dir),
            "total_figures": len(all_figures),
            "diagram_figures": 0,
            "diagram_figures_list": [],
            "results": []
        }
    
    print(f"âœ… GPTè¯†åˆ«å‡º {len(diagram_figures)} ä¸ªdiagramå›¾ç‰‡")
    
    # åˆ†ææ¯ä¸ªdiagramå›¾ç‰‡
    results = []
    for i, figure in enumerate(diagram_figures, 1):
        print(f"\nğŸ“Š åˆ†ædiagramå›¾ç‰‡ {i}: {figure['id']}")
        print(f"   Caption: {figure['caption'][:100]}...")
        
        # æ„å»ºå›¾ç‰‡è·¯å¾„
        image_path = paper_dir / "vlm" / figure['src']
        
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not image_path.exists():
            print(f"   âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
            possible_paths = [
                paper_dir / "vlm" / "images" / figure['src'],
                paper_dir / "vlm" / figure['src'].replace('images/', ''),
                paper_dir / figure['src'],
                paper_dir / "images" / figure['src']
            ]
            
            found = False
            for possible_path in possible_paths:
                if possible_path.exists():
                    image_path = possible_path
                    found = True
                    print(f"   âœ… æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶: {image_path}")
                    break
            
            if not found:
                print(f"   âŒ æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡è·¯å¾„éƒ½ä¸å­˜åœ¨")
                continue
        
        # ä½¿ç”¨æ£€ç´¢æå–ç›¸å…³ä¸Šä¸‹æ–‡
        full_context = get_semantic_context_for_figure(markdown_content, figure['caption'])
        
        # è¾“å‡ºæå–åˆ°çš„ä¸Šä¸‹æ–‡ç”¨äºè°ƒè¯•
        print(f"   ğŸ“„ æå–åˆ°çš„ä¸Šä¸‹æ–‡:")
        print(f"   {'='*50}")
        print(f"   {full_context[:300]}...")
        print(f"   {'='*50}")
        
        # ä½¿ç”¨GPT-4oåˆ†æå›¾ç‰‡
        print("   ğŸ” ä½¿ç”¨GPT-4oåˆ†æå›¾ç‰‡...")
        diagram_analysis = reasoner.analyze_diagram_with_gpt4o(str(image_path), figure['caption'], full_context)
        
        if "error" in diagram_analysis:
            print(f"   âŒ å›¾ç‰‡åˆ†æå¤±è´¥: {diagram_analysis['error']}")
            # å¦‚æœåªæ˜¯JSONè§£æå¤±è´¥ï¼Œä½†ä»ç„¶æœ‰åŸå§‹å“åº”ï¼Œå¯ä»¥ç»§ç»­å¤„ç†
            if diagram_analysis.get("raw_response"):
                print(f"   âš ï¸ ä½†æœ‰åŸå§‹å“åº”å¯ç”¨ï¼Œç»§ç»­å¤„ç†...")
            else:
                continue
        
        print("   âœ… å›¾ç‰‡åˆ†æå®Œæˆ")
        
        # ä½¿ç”¨GPT-o3ç”Ÿæˆç»˜å›¾æŒ‡ä»¤
        print("   ğŸ” ä½¿ç”¨GPT-o3ç”Ÿæˆç»˜å›¾æŒ‡ä»¤...")
        summary_result = generate_diagram_description_with_o3(
            figure['caption'], 
            full_context
        )
        
        if "error" in summary_result:
            print(f"   âŒ ç»˜å›¾æŒ‡ä»¤ç”Ÿæˆå¤±è´¥: {summary_result['error']}")
            # å³ä½¿GPT-o3å¤±è´¥ï¼Œä¹Ÿä¿å­˜GPT-4oçš„åˆ†æç»“æœ
            summary_result = {"error": "GPT-o3 summary generation failed", "diagram_analysis_available": True}
        
        print("   âœ… ç»˜å›¾æŒ‡ä»¤ç”Ÿæˆå®Œæˆ")
        
        # è§£æGPT-o3çš„JSONå“åº”
        parsed_summary = {}
        if "summary" in summary_result:
            try:
                parsed_summary = json.loads(summary_result["summary"])
                print(f"   âœ… JSONè§£ææˆåŠŸï¼ŒåŒ…å«å­—æ®µ: {list(parsed_summary.keys())}")
            except json.JSONDecodeError as e:
                print(f"   âš ï¸ JSONè§£æå¤±è´¥: {e}")
                # å°è¯•ä»markdownä»£ç å—ä¸­æå–
                parsed_summary = extract_json_from_markdown(summary_result["summary"])
                print(f"   ğŸ“‹ æå–ç»“æœ: {parsed_summary}")
        
        # æ„å»ºåŒé˜¶æ®µè®­ç»ƒæ•°æ®
        # ç¬¬ä¸€é˜¶æ®µè¾“å‡º = ç¬¬äºŒé˜¶æ®µè¾“å…¥ (diagram description)
        analysis = diagram_analysis.get("diagram_analysis", {})
        nodes = analysis.get("nodes", [])
        relationships = analysis.get("relationships", [])
        
        # ä»nodesä¸­æå–main_componentsï¼ˆä½¿ç”¨æ–°çš„nodesç»“æ„ï¼‰
        main_components = []
        for node in nodes:
            if isinstance(node, dict):
                # ä¼˜å…ˆä½¿ç”¨contentï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨id
                content = node.get("content", node.get("id", ""))
                if content:
                    main_components.append(content)
            else:
                # å‘åå…¼å®¹ï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥æ·»åŠ 
                main_components.append(str(node))
        
        diagram_description = {
            "diagram_description": parsed_summary.get("diagram_description_long", ""),
            "diagram_description_long": parsed_summary.get("diagram_description_long", ""),
            "diagram_description_short": parsed_summary.get("diagram_description_short", ""),
            "diagram_type": analysis.get("diagram_type", ""),
            "main_components": main_components,
            "relationships": relationships
        }
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        data_quality = "valid"
        quality_issues = []
        
        # æ£€æŸ¥stage1æ•°æ®è´¨é‡
        if not diagram_description.get("diagram_description", "").strip():
            data_quality = "invalid"
            quality_issues.append("empty_diagram_description")
        
        # æ£€æŸ¥stage2æ•°æ®è´¨é‡
        if "error" in summary_result:
            data_quality = "invalid"
            quality_issues.append("stage2_generation_failed")
        
        # ä»GPT-o3ç»“æœä¸­è·å–é•¿çŸ­ä¸¤ä¸ªç‰ˆæœ¬çš„æè¿°
        diagram_desc_long = diagram_description.get("diagram_description_long", diagram_description.get("diagram_description", ""))
        diagram_desc_short = diagram_description.get("diagram_description_short", "")
        
        training_data = {
            # æ•°æ®è´¨é‡æ ‡ç­¾
            "data_quality": data_quality,
            "quality_issues": quality_issues,
            
            # ç¬¬ä¸€é˜¶æ®µè®­ç»ƒæ•°æ®: context + caption -> diagram description (GPT-o3)
            "stage1_input": {
                "context": full_context,
                "caption": figure['caption']
            },
            
            # ç¬¬äºŒé˜¶æ®µè®­ç»ƒæ•°æ®: diagram description -> thinking + image (GPT-4o)
            "stage2_input": {
                "diagram_description_long": diagram_desc_long,  # é•¿ç‰ˆæœ¬æè¿°
                "diagram_description_short": diagram_desc_short  # çŸ­ç‰ˆæœ¬æè¿°ï¼ˆé€šè¿‡GPTç”Ÿæˆï¼‰
            },
            "stage2_output": {
                "thinking_long": "",
                "thinking_short": "",
                "image_path": str(image_path)  # ç¬¬äºŒé˜¶æ®µè¾“å‡ºåŒ…å«å›¾ç‰‡è·¯å¾„
            }
        }
        
        # ä½¿ç”¨GPT-o3ç”Ÿæˆthinkingï¼Œç»¼åˆè§†è§‰åˆ†æå’Œpaper context
        print(f"   ğŸ” ä½¿ç”¨GPT-o3ç”Ÿæˆthinking...")
        thinking_result = generate_thinking_with_o3(
            figure['caption'], 
            full_context, 
            json.dumps(diagram_analysis, ensure_ascii=False)
        )
        
        judge_data_entry = None
        if "summary" in thinking_result:
            thinking_content = thinking_result["summary"]
            
            # è§£æGPT-o3çš„JSONå“åº”
            try:
                thinking_json = json.loads(thinking_content)
                thinking_long = thinking_json.get("thinking_long", "")
                thinking_short = thinking_json.get("thinking_short", "")
                print(f"   âœ… Thinkingç”ŸæˆæˆåŠŸ")
            except json.JSONDecodeError as e:
                print(f"   âš ï¸ Thinking JSONè§£æå¤±è´¥: {e}")
                # å°è¯•ä»markdownä»£ç å—ä¸­æå–
                thinking_json = extract_json_from_markdown(thinking_content)
                thinking_long = thinking_json.get("thinking_long", "")
                thinking_short = thinking_json.get("thinking_short", "")
            
            # æ£€æŸ¥thinkingæ˜¯å¦ä¸ºç©ºæˆ–åŒ…å«çœç•¥å·
            if not thinking_long.strip() or "..." in thinking_long:
                data_quality = "invalid"
                quality_issues.append("empty_or_incomplete_thinking_long")
            
            if not thinking_short.strip() or "..." in thinking_short:
                data_quality = "invalid"
                quality_issues.append("empty_or_incomplete_thinking_short")
            
            training_data["stage2_output"]["thinking_long"] = thinking_long
            training_data["stage2_output"]["thinking_short"] = thinking_short
            
            # æ›´æ–°æ•°æ®è´¨é‡æ ‡ç­¾
            training_data["data_quality"] = data_quality
            training_data["quality_issues"] = quality_issues
            
            # ä¸ºjudge_dataå‡†å¤‡è¯„åˆ†æ ‡å‡†(rubric)æ•°æ®
            analysis = diagram_analysis.get("diagram_analysis", {})
            nodes = analysis.get("nodes", [])
            groups = analysis.get("groups", [])
            relationships = analysis.get("relationships", [])
            visual_elements = analysis.get("visual_elements", {})
            standalone_nodes = analysis.get("standalone_nodes", [])
            
            # æå–æ‰€æœ‰èŠ‚ç‚¹ï¼ˆä½¿ç”¨æ–°çš„nodesç»“æ„ï¼‰
            all_nodes = []
            
            # ä»nodesæ•°ç»„ä¸­æå–èŠ‚ç‚¹ä¿¡æ¯
            for node in nodes:
                if isinstance(node, dict):
                    # ä½¿ç”¨æ–°çš„nodesç»“æ„ï¼šåŒ…å«id, type, content, attributes
                    node_info = {
                        "id": node.get("id", ""),
                        "type": node.get("type", ""),
                        "content": node.get("content", ""),
                        "attributes": node.get("attributes", {})
                    }
                    all_nodes.append(node_info)
                else:
                    # å‘åå…¼å®¹ï¼šå¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥æ·»åŠ 
                    all_nodes.append(node)
            
            # æ·»åŠ ç‹¬ç«‹èŠ‚ç‚¹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            for standalone_node in standalone_nodes:
                if isinstance(standalone_node, str):
                    all_nodes.append(standalone_node)
                else:
                    all_nodes.append(standalone_node)
            
            # ä»groupsä¸­æå–èŠ‚ç‚¹å¼•ç”¨ï¼ˆä¿æŒå¼•ç”¨å…³ç³»ï¼‰
            group_node_refs = []
            for group in groups:
                group_node_refs.extend(group.get("nodes", []))
            
            judge_data_entry = {
                "image_info": {
                    "image_path": str(image_path),
                    "figure_id": figure['id'],
                    "figure_src": figure['src'],
                    "figure_caption": figure['caption']
                },
                "evaluation_rubric": {
                    "semantic_criteria": {
                        "critical_entities": all_nodes,
                        "critical_relationships": relationships,
                        "hierarchical_groups": groups,  # ç›´æ¥ä½¿ç”¨æ–°çš„groupsç»“æ„
                        "data_flow": "Sequential processing flow",
                        "dependencies": []
                    },
                    "visual_criteria": {
                        "layout_requirements": visual_elements.get("layout", ""),
                        "color_scheme": visual_elements.get("colors", []),
                        "shape_requirements": visual_elements.get("shapes", [])
                    }
                },
                "reference_descriptions": {
                    "detailed_thinking": thinking_long,
                    "concise_thinking": thinking_short
                }
            }
        
        # æ„å»ºjudgeæ•°æ®
        judge_data = judge_data_entry if judge_data_entry else {
            "image_info": {
                "image_path": str(image_path),
                "figure_id": figure['id'],
                "figure_src": figure['src'],
                "figure_caption": figure['caption']
            },
            "evaluation_rubric": {
                "semantic_criteria": {
                    "critical_entities": diagram_analysis.get("diagram_analysis", {}).get("nodes", []),
                    "critical_relationships": diagram_analysis.get("diagram_analysis", {}).get("relationships", []),
                    "hierarchical_groups": diagram_analysis.get("diagram_analysis", {}).get("groups", []),
                    "data_flow": "Sequential processing flow",
                    "dependencies": []
                },
                "visual_criteria": {
                    "layout_requirements": "",
                    "color_scheme": [],
                    "shape_requirements": []
                }
            },
            "reference_descriptions": {
                "detailed_thinking": "",
                "concise_thinking": ""
            }
        }
        
        result = {
            "training_data": training_data,
            "judge_data": judge_data
        }
        results.append(result)
    
    return {
        "paper_name": paper_name,
        "paper_dir": str(paper_dir),
        "total_figures": len(all_figures),
        "diagram_figures": len(diagram_figures),
        "diagram_figures_list": diagram_figures,
        "results": results
    }


def get_semantic_context_for_figure(markdown_content: str, caption: str) -> str:
    """ä½¿ç”¨FAISSæ£€ç´¢æ‰¾åˆ°ä¸å›¾ç‰‡ç›¸å…³çš„ä¸Šä¸‹æ–‡å†…å®¹"""
    if not HAS_LANGCHAIN:
        print("   âŒ langchain æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨FAISSæ£€ç´¢")
        return ""
    
    lines = markdown_content.split('\n')
    
    # æ‰¾åˆ°é™„å½•å¼€å§‹çš„ä½ç½®ï¼ˆæ’é™¤é™„å½•å†…å®¹ï¼‰
    appendix_start_idx = len(lines)  # é»˜è®¤æ²¡æœ‰é™„å½•
    for i, line in enumerate(lines):
        line_lower = line.lower().strip()
        # æ£€æŸ¥æ˜¯å¦æ˜¯é™„å½•æ ‡é¢˜
        if (line_lower.startswith('# appendix') or 
            line_lower.startswith('## appendix') or
            line_lower.startswith('### appendix') or
            line_lower.startswith('# supplementary') or
            line_lower.startswith('## supplementary') or
            line_lower.startswith('### supplementary') or
            'appendix' in line_lower and line_lower.startswith('#')):
            appendix_start_idx = i
            break
    
    # å°†æ–‡æ¡£åˆ†å‰²æˆæ®µè½ï¼ˆåªè€ƒè™‘æ­£æ–‡éƒ¨åˆ†ï¼Œæ’é™¤é™„å½•ï¼‰
    paragraphs = []
    current_paragraph = []
    
    for i, line in enumerate(lines[:appendix_start_idx]):  # åªå¤„ç†æ­£æ–‡éƒ¨åˆ†
        line = line.strip()
        if not line:
            if current_paragraph:
                paragraphs.append(' '.join(current_paragraph))
                current_paragraph = []
        elif not line.startswith('![') and not line.startswith('#'):
            current_paragraph.append(line)
    
    # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
    if current_paragraph:
        paragraphs.append(' '.join(current_paragraph))
    
    # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ®µè½
    paragraphs = [text for text in paragraphs if len(text) > 50]
    
    if not paragraphs:
        return ""
    
    # ä½¿ç”¨FAISSæ£€ç´¢
    try:
        print("   ğŸ” ä½¿ç”¨FAISSå¯†é›†æ£€ç´¢...")
        retriever = FAISSRetriever()
        retriever.fit(paragraphs)
        
        # æ„å»ºæŸ¥è¯¢ï¼šä½¿ç”¨captionä½œä¸ºæŸ¥è¯¢ï¼Œæ‰©å±•ç›¸å…³è¯æ±‡
        query = caption
        if any(keyword in caption.lower() for keyword in ['diagram', 'architecture', 'framework', 'pipeline', 'overview', 'structure']):
            query += " system design components modules workflow process"
        print(f"   ğŸ” FAISSæŸ¥è¯¢: {query[:100]}...")
        
        # æœç´¢æœ€ç›¸å…³çš„æ®µè½
        results = retriever.search(query, top_k=5)
        
        # æå–ç›¸å…³æ®µè½
        relevant_contexts = []
        for doc_idx, score in results:
            if score > 0:  # åªä¿ç•™æœ‰åˆ†æ•°çš„ç»“æœ
                text = paragraphs[doc_idx]
                relevant_contexts.append(text)
                print(f"   ğŸ“„ æ‰¾åˆ°ç›¸å…³æ®µè½ (åˆ†æ•°: {score:.3f}): {text[:100]}...")
        
        return '\n\n'.join(relevant_contexts)
        
    except Exception as e:
        print(f"   âŒ FAISSæ£€ç´¢å¤±è´¥: {e}")
        return ""


class FAISSRetriever:
    """åŸºäºFAISSçš„å¯†é›†å‘é‡æ£€ç´¢å™¨"""
    
    # ç±»çº§åˆ«çš„æ¨¡å‹ç¼“å­˜ï¼Œæ‰€æœ‰å®ä¾‹å…±äº«åŒä¸€ä¸ªæ¨¡å‹
    _shared_model = None
    _shared_embeddings = None
    
    def __init__(self):
        self.vector_store = None
        self.documents = []
        self.embeddings = None
        
    def _get_embeddings(self):
        """è·å–embeddingæ¨¡å‹ - å¼ºåˆ¶ä½¿ç”¨SentenceTransformeré¿å…APIé™åˆ¶"""
        if self.embeddings is not None:
            return self.embeddings
        
        # å¼ºåˆ¶ä½¿ç”¨SentenceTransformeræœ¬åœ°æ¨¡å‹ï¼ˆé¿å…Azure API rateé™åˆ¶ï¼‰
        if not HAS_SENTENCE_TRANSFORMERS:
            raise ImportError(
                "âŒ SentenceTransformer æœªå®‰è£…ï¼\n"
                "è¯·å®‰è£…: pip install sentence-transformers torch\n"
                "æˆ‘ä»¬å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹ä»¥é¿å…Azure APIé™åˆ¶ã€‚"
            )
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰å…±äº«çš„æ¨¡å‹
            if FAISSRetriever._shared_embeddings is not None:
                self.embeddings = FAISSRetriever._shared_embeddings
                print(f"   ğŸ”— å¤ç”¨å…±äº«çš„ SentenceTransformer Embeddings")
                return self.embeddings
            
            # é¦–æ¬¡åŠ è½½æ¨¡å‹
            print("   ğŸš€ å¼ºåˆ¶ä½¿ç”¨ SentenceTransformer æœ¬åœ°æ¨¡å‹...")
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"   ğŸ“± æ£€æµ‹åˆ°è®¾å¤‡: {device}")
            
            model = SentenceTransformer('all-mpnet-base-v2', device=device)
            print(f"   âœ… SentenceTransformer æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # åˆ›å»ºè‡ªå®šä¹‰çš„embeddingsç±»æ¥é€‚é…langchain
            try:
                from langchain.embeddings.base import Embeddings
            except ImportError:
                from langchain_core.embeddings import Embeddings
            
            class SentenceTransformerEmbeddings(Embeddings):
                    def __init__(self, model):
                        self.model = model
                    
                    def embed_documents(self, texts):
                        """åµŒå…¥æ–‡æ¡£åˆ—è¡¨"""
                        return self.model.encode(texts).tolist()
                    
                    def embed_query(self, text):
                        """åµŒå…¥å•ä¸ªæŸ¥è¯¢"""
                        return self.model.encode([text])[0].tolist()
            
            # åˆ›å»ºembeddingså®ä¾‹å¹¶ç¼“å­˜åˆ°ç±»çº§åˆ«
            FAISSRetriever._shared_model = model
            FAISSRetriever._shared_embeddings = SentenceTransformerEmbeddings(model)
            self.embeddings = FAISSRetriever._shared_embeddings
            
            print(f"   ğŸ”— é¦–æ¬¡åŠ è½½ SentenceTransformer Embeddings (è®¾å¤‡: {device})")
            print(f"   ğŸ’¾ æ¨¡å‹å·²ç¼“å­˜ï¼Œåç»­å®ä¾‹å°†å¤ç”¨æ­¤æ¨¡å‹")
            print(f"   ğŸš« å·²ç¦ç”¨Azure APIï¼Œé¿å…rateé™åˆ¶é—®é¢˜")
            return self.embeddings
            
        except Exception as e:
            error_msg = (
                f"âŒ SentenceTransformer åˆå§‹åŒ–å¤±è´¥: {e}\n"
                f"ğŸ”§ è§£å†³æ–¹æ¡ˆ:\n"
                f"   1. ç¡®ä¿å·²å®‰è£…: pip install sentence-transformers torch\n"
                f"   2. æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½æ¨¡å‹ï¼‰\n"
                f"   3. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´\n"
                f"   4. å¦‚æœä½¿ç”¨GPUï¼Œç¡®ä¿CUDAç¯å¢ƒæ­£ç¡®é…ç½®\n"
                f"ğŸš« æˆ‘ä»¬ä¸å†ä½¿ç”¨Azure APIä»¥é¿å…rateé™åˆ¶é—®é¢˜"
            )
            print(error_msg)
            raise RuntimeError(error_msg)
    
    def fit(self, documents: List[str]):
        """æ„å»ºFAISSç´¢å¼•"""
        if not HAS_LANGCHAIN:
            raise ImportError("éœ€è¦å®‰è£…langchainæ¥ä½¿ç”¨FAISSæ£€ç´¢")
        
        self.documents = documents
        
        # åˆ›å»ºDocumentå¯¹è±¡
        docs = [Document(page_content=doc) for doc in documents]
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = text_splitter.split_documents(docs)
        
        # è·å–embeddingså¹¶æ„å»ºFAISSç´¢å¼•
        embeddings = self._get_embeddings()
        self.vector_store = FAISS.from_documents(chunks, embeddings)
        
        print(f"   ğŸ“Š FAISSç´¢å¼•æ„å»ºå®Œæˆ: {len(chunks)} ä¸ªchunks")
    
    def search(self, query: str, top_k: int = 3) -> List[tuple]:
        """æœç´¢æœ€ç›¸å…³çš„æ–‡æ¡£"""
        if self.vector_store is None:
            return []
        
        # ä½¿ç”¨FAISSæ£€ç´¢
        retriever = self.vector_store.as_retriever(search_kwargs={"k": top_k})
        try:
            # ä½¿ç”¨æ–°çš„invokeæ–¹æ³•
            docs = retriever.invoke(query)
        except AttributeError:
            # å›é€€åˆ°æ—§æ–¹æ³•
            docs = retriever.get_relevant_documents(query)
        
        # è¿”å›ç»“æœ
        results = []
        for i, doc in enumerate(docs):
            # æ‰¾åˆ°åŸå§‹æ–‡æ¡£çš„ç´¢å¼•
            original_idx = self._find_original_doc_index(doc.page_content)
            if original_idx != -1:
                results.append((original_idx, 1.0 - i * 0.1))  # ç®€å•çš„åˆ†æ•°è®¡ç®—
        
        return results
    
    def _find_original_doc_index(self, content: str) -> int:
        """æ‰¾åˆ°chunkå¯¹åº”çš„åŸå§‹æ–‡æ¡£ç´¢å¼•"""
        for i, doc in enumerate(self.documents):
            if content[:100] in doc:  # ä½¿ç”¨å‰100ä¸ªå­—ç¬¦åŒ¹é…
                return i
        return -1


def main():
    """ä¸»å‡½æ•° - æµ‹è¯•æ‰€æœ‰markdownè®ºæ–‡çš„æ™ºèƒ½diagramåˆ†æ"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ··åˆå›¾è¡¨åˆ†æå™¨ - æ”¯æŒå¤šAPIæº")
    parser.add_argument("--dedupe", action="store_true", default=True,
                        help="æŒ‰arxiv IDå»é‡ï¼Œåªå¤„ç†æ¯ä¸ªè®ºæ–‡çš„æœ€æ–°ç‰ˆæœ¬ (é»˜è®¤: True)")
    parser.add_argument("--no-dedupe", action="store_true", 
                        help="ä¸å»é‡ï¼Œå¤„ç†æ‰€æœ‰ç‰ˆæœ¬")
    parser.add_argument("--paper-id", type=str, 
                        help="åªå¤„ç†æŒ‡å®šçš„arxiv ID (ä¾‹å¦‚: 1905.12185)")
    
    args = parser.parse_args()
    
    # å¤„ç†å»é‡å‚æ•°
    dedupe = args.dedupe and not args.no_dedupe
    
    print("ğŸ¤– æ··åˆå›¾è¡¨åˆ†æå™¨ - æ”¯æŒå¤šAPIæº")
    print("=" * 60)
    print(f"ğŸ“š å»é‡æ¨¡å¼: {'å¯ç”¨ (é€‰æ‹©æœ€æ–°ç‰ˆæœ¬)' if dedupe else 'ç¦ç”¨'}")
    if args.paper_id:
        print(f"ğŸ¯ æŒ‡å®šè®ºæ–‡ID: {args.paper_id}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰APIé…ç½®
    has_azure_key = bool(os.getenv("AZURE_OPENAI_API_KEY"))
    has_azure_identity = HAS_AZURE_IDENTITY
    
    if not has_azure_key and not has_azure_identity:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•APIé…ç½®")
        show_setup_help()
        return
    
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    reasoner = HybridDiagramReasoner(APISource.PAPYRUS)
    
    # æµ‹è¯•APIè¿æ¥
    if not reasoner.test_api_connection():
        print("âŒ APIè¿æ¥å¤±è´¥")
        return
    
    # è®ºæ–‡ç›®å½•
    papers_dir = Path(__file__).parent.parent.parent / "workspace" / "papers_markdown"
    
    # è·å–æ‰€æœ‰è®ºæ–‡ç›®å½•
    all_paper_dirs = [d for d in papers_dir.iterdir() if d.is_dir()]
    
    if not all_paper_dirs:
        print("âŒ æœªæ‰¾åˆ°è®ºæ–‡ç›®å½•")
        return
    
    # å¤„ç†è®ºæ–‡é€‰æ‹©é€»è¾‘
    if args.paper_id:
        # åªå¤„ç†æŒ‡å®šçš„è®ºæ–‡ID
        paper_dirs = []
        for paper_dir in all_paper_dirs:
            paper_name = paper_dir.name
            import re
            match = re.match(r'(\d{4}\.\d{4,5})', paper_name)
            if match and match.group(1) == args.paper_id:
                paper_dirs.append(paper_dir)
        
        if not paper_dirs:
            print(f"âŒ æœªæ‰¾åˆ°æŒ‡å®šçš„è®ºæ–‡ID: {args.paper_id}")
            return
        
        print(f"ğŸ“š æ‰¾åˆ° {len(paper_dirs)} ä¸ªåŒ¹é…çš„è®ºæ–‡ç‰ˆæœ¬")
        
    elif dedupe:
        # æŒ‰arxiv IDå»é‡ï¼Œåªä¿ç•™æ¯ä¸ªè®ºæ–‡çš„ç¬¬ä¸€ä¸ªç‰ˆæœ¬
        paper_groups = {}
        for paper_dir in all_paper_dirs:
            paper_name = paper_dir.name
            # æå–arxiv ID (ä¾‹å¦‚: 1905.12185v3 -> 1905.12185)
            import re
            match = re.match(r'(\d{4}\.\d{4,5})', paper_name)
            if match:
                arxiv_id = match.group(1)
                if arxiv_id not in paper_groups:
                    paper_groups[arxiv_id] = []
                paper_groups[arxiv_id].append(paper_dir)
        
        # æ¯ä¸ªarxiv IDåªé€‰æ‹©æœ€åä¸€ä¸ªç‰ˆæœ¬ï¼ˆæœ€æ–°ç‰ˆæœ¬ï¼‰
        paper_dirs = []
        for arxiv_id, versions in paper_groups.items():
            # æŒ‰ç‰ˆæœ¬æ’åºï¼Œé€‰æ‹©æœ€åä¸€ä¸ªï¼ˆæœ€æ–°ç‰ˆæœ¬ï¼‰
            versions.sort()
            selected_version = versions[-1]
            paper_dirs.append(selected_version)
            if len(versions) > 1:
                print(f"ğŸ“š {arxiv_id}: æ‰¾åˆ° {len(versions)} ä¸ªç‰ˆæœ¬ï¼Œé€‰æ‹©æœ€æ–°ç‰ˆæœ¬ {selected_version.name}")
        
        print(f"ğŸ“š å»é‡åæ‰¾åˆ° {len(paper_dirs)} ä¸ªå”¯ä¸€è®ºæ–‡")
        
    else:
        # ä¸å»é‡ï¼Œå¤„ç†æ‰€æœ‰ç‰ˆæœ¬
        paper_dirs = all_paper_dirs
        print(f"ğŸ“š å¤„ç†æ‰€æœ‰ {len(paper_dirs)} ä¸ªè®ºæ–‡ç‰ˆæœ¬")
    
    # æµ‹è¯•æ¯ä¸ªè®ºæ–‡
    all_results = []
    for paper_dir in paper_dirs:
        paper_name = paper_dir.name
        result = test_smart_markdown_paper(paper_dir, paper_name, reasoner)
        if result:
            all_results.append(result)
    
    # åˆ†ç¦»è®­ç»ƒæ•°æ®å’Œjudgeæ•°æ®
    training_data = []
    judge_data = []
    
    for result in all_results:
        for item in result["results"]:
            if item.get("training_data"):
                training_data.append(item["training_data"])
            if item.get("judge_data"):
                judge_data.append(item["judge_data"])
    
    # ä¿å­˜è®­ç»ƒæ•°æ®
    training_output_path = Path(__file__).parent / "hybrid_diagram_training_data.json"
    with open(training_output_path, 'w', encoding='utf-8') as f:
        json.dump(training_data, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜judgeæ•°æ®
    judge_output_path = Path(__file__).parent / "hybrid_diagram_judge_data.json"
    with open(judge_output_path, 'w', encoding='utf-8') as f:
        json.dump(judge_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {training_output_path}")
    print(f"ğŸ’¾ Judgeæ•°æ®å·²ä¿å­˜åˆ°: {judge_output_path}")
    
    # ç»Ÿè®¡ç»“æœ
    print(f"\n{'='*60}")
    print("ğŸ“Š æ··åˆåˆ†æå™¨ç»“æœç»Ÿè®¡")
    print(f"{'='*60}")
    
    total_papers = len(all_results)
    total_figures = sum(r["total_figures"] for r in all_results)
    total_diagrams = sum(r["diagram_figures"] for r in all_results)
    successful_analyses = 0
    valid_training_data = 0
    invalid_training_data = 0
    
    for result in all_results:
        paper_name = result["paper_name"]
        total_figs = result["total_figures"]
        diagram_figs = result["diagram_figures"]
        successful = len(result["results"])
        
        # ç»Ÿè®¡æ•°æ®è´¨é‡
        valid_count = sum(1 for r in result["results"] if r.get("training_data", {}).get("data_quality") == "valid")
        invalid_count = sum(1 for r in result["results"] if r.get("training_data", {}).get("data_quality") == "invalid")
        
        print(f"ğŸ“š {paper_name}:")
        print(f"   ğŸ“Š æ€»å›¾ç‰‡æ•°: {total_figs}")
        print(f"   ğŸ¤– GPTè¯†åˆ«diagram: {diagram_figs}")
        print(f"   âœ… æˆåŠŸåˆ†æ: {successful}/{diagram_figs}")
        print(f"   âœ… æœ‰æ•ˆè®­ç»ƒæ•°æ®: {valid_count}")
        print(f"   âŒ æ— æ•ˆè®­ç»ƒæ•°æ®: {invalid_count}")
        print()
        
        successful_analyses += successful
        valid_training_data += valid_count
        invalid_training_data += invalid_count
    
    print(f"ğŸ¯ æ€»ä½“ç»Ÿè®¡:")
    print(f"   ğŸ“š æµ‹è¯•è®ºæ–‡æ•°: {total_papers}")
    print(f"   ğŸ“Š æ€»å›¾ç‰‡æ•°: {total_figures}")
    print(f"   ğŸ¤– GPTè¯†åˆ«diagram: {total_diagrams}")
    print(f"   âœ… æˆåŠŸåˆ†æ: {successful_analyses}/{total_diagrams}")
    print(f"   ğŸ“ˆ æˆåŠŸç‡: {successful_analyses/total_diagrams*100:.1f}%" if total_diagrams > 0 else "   ğŸ“ˆ æˆåŠŸç‡: N/A")
    print(f"   ğŸ¯ è¯†åˆ«ç‡: {total_diagrams/total_figures*100:.1f}%" if total_figures > 0 else "   ğŸ¯ è¯†åˆ«ç‡: N/A")
    print(f"   âœ… æœ‰æ•ˆè®­ç»ƒæ•°æ®: {valid_training_data}")
    print(f"   âŒ æ— æ•ˆè®­ç»ƒæ•°æ®: {invalid_training_data}")
    print(f"   ğŸ“Š æ•°æ®è´¨é‡ç‡: {valid_training_data/(valid_training_data+invalid_training_data)*100:.1f}%" if (valid_training_data+invalid_training_data) > 0 else "   ğŸ“Š æ•°æ®è´¨é‡ç‡: N/A")
    
    print(f"\nğŸ’¾ å®Œæ•´è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {training_output_path}")
    print(f"âœ… æœ€ç»ˆä½¿ç”¨çš„APIæº: {reasoner.api_source.value}")


if __name__ == "__main__":
    import json
    main()
